[{"title":"minisys-cpu","path":"/2024/12/25/fpga/minisys-cpu/","content":"控制模块 该模块按照31条指令表对照填写即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546`timescale 1ns / 1ps//////////////////////////////////////////////////////////////////////////////////module control32 ( input\t[5:0] Opcode, // 来自取指单元instruction[31..26] input\t[5:0] Function_opcode,\t// 来自取指单元r-类型 instructions[5..0] output Jrn, // 为1表明当前指令是jr output RegDST, // 为1表明目的寄存器是rd，否则目的寄存器是rt output ALUSrc, // 为1表明第二个操作数是立即数（beq，bne除外） output MemtoReg, // 为1表明需要从存储器读数据到寄存器 output RegWrite, // 为1表明该指令需要写寄存器 output MemWrite, // 为1表明该指令需要写存储器 output Branch, // 为1表明是Beq指令 output nBranch, // 为1表明是Bne指令 output Jmp, // 为1表明是J指令 output Jal, // 为1表明是Jal指令 output I_format, // 为1表明该指令是除beq，bne，LW，SW之外的其他I-类型指令 output Sftmd, // 为1表明是移位指令 output\t[1:0]\tALUOp // 是R-类型或I_format=1时位1为1, beq、bne指令则位0为1); wire R_format; // 为1表示是R-类型指令 wire Lw; // 为1表示是lw指令 wire Sw; // 为1表示是sw指令 assign R_format = (Opcode==6&#x27;b000000)? 1&#x27;b1:1&#x27;b0; //--00h assign RegDST = R_format; //说明目标是rd，否则是rt assign I_format = (Opcode[5:3] == 3&#x27;b001) ? 1&#x27;b1 : 1&#x27;b0; assign Lw = (Opcode == 6&#x27;b100011) ? 1&#x27;b1 : 1&#x27;b0; assign Jal = (Opcode == 6&#x27;b000011) ? 1&#x27;b1 : 1&#x27;b0; assign Jrn = (Opcode == 6&#x27;b000000 &amp;&amp; Function_opcode == 6&#x27;b001000) ? 1&#x27;b1 : 1&#x27;b0; assign RegWrite = (R_format | I_format | Jal | Lw) &amp; (!Jrn); assign Sw = (Opcode == 6&#x27;b101011) ? 1&#x27;b1 : 1&#x27;b0; assign ALUSrc = (I_format | Lw | Sw); assign Branch = (Opcode == 6&#x27;b000100) ? 1&#x27;b1 : 1&#x27;b0; assign nBranch = (Opcode == 6&#x27;b000101) ? 1&#x27;b1 : 1&#x27;b0; assign Jmp = (Opcode == 6&#x27;b000010) ? 1&#x27;b1 : 1&#x27;b0; assign MemWrite = Sw; assign MemtoReg = Lw; assign Sftmd = (Opcode == 6&#x27;b000000 &amp;&amp; Function_opcode[5:3] == 3&#x27;b000) ? 1&#x27;b1 : 1&#x27;b0; assign ALUOp = &#123;(R_format || I_format),(Branch || nBranch)&#125;; // 是R－type或需要立即数作32位扩展的指令1位为1,beq、bne指令则0位为1endmodule 模拟仿真 取指模块 接口部分 123456789101112131415161718module Ifetc32 (\tinput reset, // 复位信号(高电平有效) input clock, // 时钟(23MHz)\toutput\t[31:0]\tInstruction, // 输出指令到其他模块 output\t[31:0]\tPC_plus_4_out, // (pc+4)送执行单元 input\t[31:0]\tAdd_result, // 来自执行单元,算出的跳转地址 input\t[31:0]\tRead_data_1, // 来自译码单元，jr指令用的地址 input Branch, // 来自控制单元 beq指令 input nBranch, // 来自控制单元 bne指令 input Jmp, // 来自控制单元 J指令 input Jal, // 来自控制单元 jal指令 input Jrn, // 来自控制单元 jr指令 input Zero, // 来自执行单元 为1表示计算值为0 output\t[31:0]\topcplus4, // JAL指令专用的PC+4 // ROM Pinouts\toutput\t[13:0]\trom_adr_o, // 给程序ROM单元的取指地址\tinput\t[31:0]\tJpadr // 从程序ROM单元中获取的指令); PC+4处理 直接对高30位加1即可。 123assign PC_plus_4[31:2] =PC[31:2] + 1&#x27;b1;assign PC_plus_4[1:0] = PC[1:0];assign PC_plus_4_out = PC[31:0]; beq，bne，jr 跳转对 PC 的修改 指令 指令描述 beq if ((rt)=(rs)) then (PC)←(PC)+4+( (Sign-Extend) offset&lt;&lt;2), rs=$1, rt=$2 bne if ((rt)≠(rs)) then (PC)←(PC)+4+((Sign-Extend) offset&lt;&lt;2) , rs=$1, rt=$2 jr (PC)←(rs) 跳转地址由inputinputinput接口给出： 12input\t[31:0]\tAdd_result, // 来自执行单元,算出的跳转地址input\t[31:0]\tRead_data_1, // 来自译码单元，jr指令用的地址 123456789101112131415always @* begin // beq $n ,$m if $n=$m branch bne if $n /=$m branch jr // 请考虑以上三条指令的判断条件， // 以及三条指令的执行该给next_PC赋什么值 // beq : 相等则跳转-&gt;pc+4+(offset&lt;&lt;2) if(Branch == 1&#x27;b1 &amp;&amp; Zero == 1&#x27;b1) next_PC = Add_result &lt;&lt; 2; // bne : 不相等则跳转 else if(nBranch == 1&#x27;b1 &amp;&amp; Zero == 1&#x27;b0) next_PC = Add_result &lt;&lt; 2; // jr : 无条件跳转-&gt; $31 else if(Jrn == 1&#x27;b1) next_PC = Read_data_1 &lt;&lt; 2; else next_PC = PC_plus_4;end 最终修改 PC 值 指令 指令描述 J (PC)←( (Zero-Extend) address&lt;&lt;2) Jal ($31)←(PC)+4; (PC)←( (Zero-Extend) address&lt;&lt;2) 123456789101112always @(negedge clock) begin //（含J，Jal指令和reset的处理） if(reset == 1&#x27;b1) begin PC = 32&#x27;b0; opcplus4 = 32&#x27;b0; end else if(Jmp == 1&#x27;b1) // J PC = Instruction[25:0] &lt;&lt; 2; else if(Jal == 1&#x27;b1) begin //Jal opcplus4 = PC_plus_4; PC = Instruction[25:0] &lt;&lt; 2; end else PC = next_PC;end 模拟仿真 抽象之处：实验报告中的仿真文件与minisys-cpu项目给出的仿真文件不同（400ns处），导致使用项目仿真文件与实验报告仿真图不一致（下为修改仿真文件后的仿真图） 译码模块 接口模块 123456789101112131415module Idecode32 (\tinput reset, input clock, output\t[31:0]\tread_data_1,\t// 输出的第一操作数 output\t[31:0]\tread_data_2,\t// 输出的第二操作数 input\t[31:0]\tInstruction,\t// 取指单元来的指令 input\t[31:0]\tread_data, // 从DATA RAM or I/O port取出的数据 input\t[31:0]\tALU_result, // 从执行单元来的运算的结果，需要扩展立即数到32位 input Jal, // 来自控制单元，说明是JAL指令 input RegWrite, // 来自控制单元 input MemtoReg, // 来自控制单元 input RegDst, // 来自控制单元 output\t[31:0]\tSign_extend,\t// 译码单元输出的扩展后的32位立即数 input\t[31:0]\topcplus4 // 来自取指单元，JAL中用); 指令分量分离 根据Minisys−1Minisys-1Minisys−1指令格式，对各部分进行分离即可。 123456assign opcode = Instruction[31:26]; //OPassign read_register_1_address = Instruction[25:21]; //rs assign read_register_2_address = Instruction[20:16]; //rt assign write_register_address_1 = Instruction[15:11];\t// rd(r-form)assign write_register_address_0 = Instruction[20:16]; //rt(i-form)assign Instruction_immediate_value = Instruction[15:0]; //data,rladr(i-form) 16位立即数扩展 需要区分该指令需要零扩展还是符号扩展。有立即数的III指令中，只有4条进行零扩展的指令，将他们的扩展位为设为1，其余指令的扩展位设为16位立即数的符号位。 12assign sign = (opcode==6&#x27;b001100 || opcode==6&#x27;b001101 || opcode==6&#x27;b001110 || opcode==6&#x27;b001011) ? 1&#x27;b0 : Instruction_immediate_value[15];assign Sign_extend[31:0] = &#123;&#123;16&#123;sign&#125;&#125;,Instruction_immediate_value&#125;; 写寄存器地址 RegDstRegDstRegDst 记录了目的寄存器（为1表明目的寄存器是rd，否则目的寄存器是rt）。特殊的，如果为JalJalJal指令，需要将PC_Plus_4PC\\_Plus\\_4PC_Plus_4写入31号寄存器。 12345678always @* begin //这个进程指定不同指令下的目标寄存器 if (Jal) write_register_address = 5&#x27;d31; else if (RegDst) write_register_address = write_register_address_1; else write_register_address = write_register_address_0;end 准备写的数据 如果为lwlwlw指令，则将内存中读得数据写入寄存器； 如果为JalJalJal指令，则将PC_Plus_4PC\\_Plus\\_4PC_Plus_4写入31号寄存器； 否则，将ALUALUALU的运算结果写入寄存器。 12345678always @* begin //这个进程基本上是实现结构图中右下的多路选择器,准备要写的数据 if (Jal) write_data = opcplus4; else if (MemtoReg) write_data = read_data; else write_data = ALU_result;end 复位及写寄存器操作 注意：$0\\$0$0寄存器的值始终为0。 123456789integer i;always @(posedge clock) begin // 本进程写目标寄存器 if(reset==1) begin // 初始化寄存器组 for(i=0;i&lt;32;i=i+1) register[i] &lt;= 0; end else if(RegWrite==1) begin // 注意寄存器0恒等于0 register[write_register_address] &lt;= write_data; register[0] &lt;= 0; endend 模拟仿真 抽象之处：指导书中寄存器复位逻辑与资源包不符，指导书使用对第iii个寄存器赋值为iii，而资源包使用所有寄存器赋值为000。个人理解为指导书赋值为iii为了仿真方便，因此，在仿真截图后将赋值逻辑重新改回所有寄存器赋值为000。 执行模块 在控制单元设计中，为了减轻控制单元的负担，对 $ALU $采用二级控制方式，并在控制单元中仅输出 $ALUop $作为一级控制。 接口部分 1234567891011121314151617module Executs32 ( input\t[31:0]\tRead_data_1, // 从译码单元的Read_data_1中来 input\t[31:0]\tRead_data_2, // 从译码单元的Read_data_2中来 input\t[31:0]\tSign_extend, // 从译码单元来的扩展后的立即数 input\t[5:0]\tFunction_opcode,\t// 取指单元来的r-类型指令功能码,r-form instructions[5:0] input\t[5:0]\tExe_opcode, // 取指单元来的操作码 input\t[1:0]\tALUOp, // 来自控制单元的运算指令控制编码 input\t[4:0]\tShamt, // 来自取指单元的instruction[10:6]，指定移位次数 input Sftmd, // 来自控制单元的，表明是移位指令 input ALUSrc, // 来自控制单元，表明第二个操作数是立即数（beq，bne除外） input I_format,// 来自控制单元，表明是除beq, bne, LW, SW之外的I-类型指令 input Jrn, // 来自控制单元，书名是JR指令 output Zero, // 为1表明计算值为0 output\t[31:0]\tALU_Result, // 计算的数据结果 output\t[31:0]\tAdd_Result, // 计算的地址结果 input\t[31:0]\tPC_plus_4 // 来自取指单元的PC+4); 定义二级控制 SftmdSftmdSftmd：输入信号，为1说明为移位指令。而所有移位指令均为RRR类型指令，op−codeop-codeop−code均为6′b000_0006&#x27;b000\\_0006′b000_000 ，只需通过func−codefunc-codefunc−code区分，而移位指令的func−codefunc-codefunc−code高2位均为0，只需使用SftmSftmSftm记录func−codefunc-codefunc−code的低4位即可区分不同的移位指令。 AinputAinputAinput和BinputBinputBinput代表操作数1和操作数2，通常操作数1为rsrsrs寄存器，操作数2为rtrtrt寄存器或立即数。 接着使用一系列复杂逻辑，定义了ExeCodeExeCodeExeCode和AluCtrlAluCtrlAluCtrl，将指令分为了12组，可以通过这些标识(包含I_FormatI\\_FormatI_Format)辨别当前在处理哪种指令。 1234567assign Sftm = Function_opcode[2:0]; // 实际有用的只有低三位(移位指令）assign Exe_code = (I_format==0) ? Function_opcode : &#123;3&#x27;b000,Exe_opcode[2:0]&#125;;assign Ainput = Read_data_1;assign Binput = (ALUSrc == 0) ? Read_data_2 : Sign_extend[31:0]; assign ALU_ctl[0] = (Exe_code[0] | Exe_code[3]) &amp; ALUOp[1]; assign ALU_ctl[1] = ((!Exe_code[2]) | (!ALUOp[1]));assign ALU_ctl[2] = (Exe_code[1] &amp; ALUOp[1]) | ALUOp[0]; 六种移位指令 指令名称 功能码(func) 指令描述 sll 000000 (rd)←(rt)&lt;&lt;shamt srl 000010 (rd)←(rt)&gt;&gt;shamt (逻辑右移) sra 000011 (rd)←(rt)&gt;&gt;shamt (算术右移，注意符号位保留) sllv 000100 (rd)←(rt)&lt;&lt;(rs) srlv 000110 (rd)←(rt)&gt;&gt;(rs) (逻辑右移) srav 000111 (rd)←(rt)&gt;&gt;(rs) (算术右移，注意符号位保留) 12345678910111213always @* begin // 6种移位指令 if(Sftmd) case(Sftm[2:0]) 3&#x27;b000:Sinput = Binput &lt;&lt; Shamt;\t//Sll rd,rt,shamt 00000 3&#x27;b010:Sinput = Binput &gt;&gt; Shamt; //Srl rd,rt,shamt 00010 3&#x27;b100:Sinput = Binput &lt;&lt; Ainput; //Sllv rd,rt,rs 000100 3&#x27;b110:Sinput = Binput &gt;&gt; Ainput; //Srlv rd,rt,rs 000110 3&#x27;b011:Sinput = Binput &gt;&gt;&gt; Shamt; //Sra rd,rt,shamt 00011 3&#x27;b111:Sinput = Binput &gt;&gt;&gt; Ainput;\t//Srav rd,rt,rs 00111 default:Sinput = Binput; endcase else Sinput = Binput;end SLT类问题 指令 指令描述 slti if ((rs)&lt;(Sign-Extend)immediate) then (rt)←1; else (rt)←0 sltiu if ((rs)&lt;(Zero-Extend)immediate) then (rt)←1; else (rt)←0 slt if (rs&lt; rt) rd=1 else rd=0; sltu if (rs&lt; rt) rd=1 else rd=0, 无符号数 lui (rt)←immediate &lt;&lt;16 &amp; 0FFFF0000H 处理了所有根据比较结果赋值的指令和luiluilui指令，虽然此处对subsubsub指令也做了处理，但是由于subsubsub指令并不是比较指令，StfmdStfmdStfmd为0，所以并不会将处理结果给ALU_ResultALU\\_ResultALU_Result赋值，因此不会影响最终输出。 12345678910always @* begin if(((ALU_ctl==3&#x27;b111) &amp;&amp; (Exe_code[3]==1))||((ALU_ctl[2:1]==2&#x27;b11) &amp;&amp; (I_format==1))) //slti(sub) 处理所有SLT类的问题 ALU_Result = (Ainput &lt; Binput) ? 1&#x27;b1 : 1&#x27;b0; else if((ALU_ctl==3&#x27;b101) &amp;&amp; (I_format==1)) ALU_Result[31:0] = &#123;Binput[15:0],16&#x27;h0000&#125;; //lui data else if(Sftmd==1) ALU_Result = Sinput; // 移位 else ALU_Result = ALU_output_mux[31:0]; //otherwiseend 跳转指令 指令名称 指令描述 beq if ((rt)=(rs)) then (PC)←(PC)+4+( (Sign-Extend) offset&lt;&lt;2) bne if ((rt)≠(rs)) then (PC)←(PC)+4+((Sign-Extend) offset&lt;&lt;2) 根据上表可知，beqbeqbeq和bnebnebne跳转指令的AluCtrlAluCtrlAluCtrl与减法指令相等，因此在后面会做rt−rsrt-rsrt−rs操作，在这里判断结果是否为0，可以作为是否执行跳转指令的依据。 1234// 给取指单元作为beq和bne指令的跳转地址 assign Add_Result = PC_plus_4[31:0] + &#123;Sign_extend[29:0],2&#x27;b00&#125;; // 判断运算结果是否为0assign Zero = (ALU_output_mux[31:0]== 32&#x27;h00000000) ? 1&#x27;b1 : 1&#x27;b0; 算数逻辑运算指令 根据AluCtrlAluCtrlAluCtrl区分不同操作，对其进行相应运算即可。 其中lwlwlw和swswsw指令为与内存交互的指令，其AluCtrlAluCtrlAluCtrl与加法指令相同，将rsrsrs与offsetoffsetoffset相加，使AluResultAluResultAluResult为读写内存的地址。 12345678910111213always @(ALU_ctl or Ainput or Binput) begin case(ALU_ctl) 3&#x27;b000:ALU_output_mux = Ainput &amp; Binput; 3&#x27;b001:ALU_output_mux = Ainput | Binput; 3&#x27;b010:ALU_output_mux = Ainput + Binput; 3&#x27;b011:ALU_output_mux = Ainput + Binput; 3&#x27;b100:ALU_output_mux = Ainput ^ Binput; 3&#x27;b101:ALU_output_mux = ~(Ainput | Binput); 3&#x27;b110:ALU_output_mux = Ainput - Binput; 3&#x27;b111:ALU_output_mux = Ainput - Binput; default:ALU_output_mux = 32&#x27;h00000000; endcaseend 模拟仿真 抽象之处：指导书beqbeqbeq指令的跳转地址计算错误，计算逻辑完全不符合指令逻辑，应为： pc_plus_4+(sign_extend&lt;&lt;2)=0X18H+(0X04H∗4)=0X28H\\ \\ \\ pc\\_plus\\_4 + (sign\\_extend &lt;&lt; 2) \\\\ = 0X18H + (0X04H * 4) = 0X28H pc_plus_4+(sign_extend&lt;&lt;2)=0X18H+(0X04H∗4)=0X28H IO模块 控制模块+IO Alu_resultHighAlu\\_resultHighAlu_resultHigh来自执行单元Alu_Result[31..10]Alu\\_Result[31..10]Alu_Result[31..10]，其输出读写内存器的地址。因此判断是否进行的是IOIOIO操作，只需判断当前指令是否为IOIOIO操作和读写内存的地址是否在IOIOIO地址空间之内。 123456// 以下为修改部分assign MemWrite = (Sw &amp;&amp; (Alu_resultHigh[21:0] !=22&#x27;b1111111111111111111111));assign MemorIOtoReg = Lw;assign MemRead = (Lw &amp;&amp; (Alu_resultHigh[21:0] != 22&#x27;b1111111111111111111111)); assign IOWrite = (Sw &amp;&amp; (Alu_resultHigh[21:0] != 22&#x27;b1111111111111111111111)); assign IORead = (Lw &amp;&amp; (Alu_resultHigh[21:0] != 22&#x27;b1111111111111111111111)); Led和Switch模块 端口的数据宽度为 16 位，因此必须将 24 个 LED 拆分为两部分，对应两个端口，其中低地址端口对应16位led灯，高地址端口的低8位对应8位led灯。switchswitchswitch模块同理。 1234567891011121314151617181920212223242526272829`timescale 1ns / 1psmodule leds ( input ledrst, // 复位信号 input led_clk,\t// 时钟信号 input ledwrite,\t// 写信号 input ledcs, // 从memorio来的，由低至高位形成的LED片选信号 input\t[1:0]\tledaddr,\t// 到LED模块的地址低端 input\t[15:0]\tledwdata,\t// 写到LED模块的数据，注意数据线只有16根 output\t[23:0]\tledout // 向板子上输出的24位LED信号); reg [23:0] ledout; always@(posedge led_clk or posedge ledrst) begin // 在此处填写LDE模块的代码 if(ledrst == 1&#x27;b1) ledout &lt;= 24&#x27;b0; else if(ledcs == 1&#x27;b1) begin if(ledcs == 1&#x27;b1) begin if(ledaddr == 2&#x27;b00) ledout[15:0] = ledwdata[15:0]; else ledout[23:16] = ledwdata[7:0]; end else ledout &lt;= 24&#x27;b0; end endendmodule 12345678910111213141516171819202122232425262728`timescale 1ns / 1psmodule switchs ( input switrst, // 复位信号 input switclk, // 时钟信号 input switchcs,\t// 从memorio来的，由低至高位形成的switch片选信号 input\t[1:0]\tswitchaddr, // 到switch模块的地址低端 input switchread, // 读信号 output\t[15:0]\tswitchrdata,\t// 送到CPU的拨码开关值注意数据总线只有16根 input\t[23:0]\tswitch_i // 从板上读的24位开关数据); reg [15:0] switchrdata; always@(negedge switclk or posedge switrst) begin // 在此处填写拨码开关模块的代码 if(switrst == 1&#x27;b1) switchrdata &lt;= 16&#x27;b0; else begin if(switchcs == 1&#x27;b1) begin if(switchaddr == 2&#x27;b0) switchrdata = switch_i[15:0]; else switchrdata[7:0] = switch_i[23:16]; end else switchrdata = 16&#x27;b0; end endendmodule","tags":["fpga"],"categories":["fpga"]},{"title":"git加速","path":"/2024/12/20/计算机基础/gitclone/","content":"gitclone加速 使用gitclone镜像网站加速 123git clone https://github.com/tendermint/tendermint.git// 变为git clone https://gitclone.com/github.com/tendermint/tendermint.git pip临时使用清华园 1pip3 install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple conda 创建环境 1conda create -n rl python=3.10 linux文件权限问题 查看文件权限 1ls -l 文件名称 给指定文件可读可写权限 1chmod 777 文件名称 给文件夹可读可写权限 1chmod -R 777 指定目录 cudatoolkit-11.3.1","tags":["git"],"categories":["计算机基础"]},{"title":"李宏毅强化学习","path":"/2024/12/17/RL/rl-by-lihongyi/","content":"策略梯度算法 1. 基础 在一场游戏里面，我们把环境输出的 s 与演员输出的动作 a 全部组合起来，就是一个轨迹，即 trajection ⁣:τ={s1,a1,s2,a2,⋯ ,st,at}trajection \\colon \\tau=\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{t}, a_{t} \\} trajection:τ={s1​,a1​,s2​,a2​,⋯,st​,at​} 给定演员的参数 θθθ，我们可以计算某个轨迹τττ发生的概率为 pθ(τ)=p(s1)pθ(a1∣s1)p(s2∣s1,a1)pθ(a2∣s2)p(s3∣s2,a2)⋯=p(s1)∏t=1Tpθ(at∣st)p(st+1∣st,at)\\begin{aligned} { {p_{\\theta} ( \\tau)} } &amp; { {} { {} {} { {}=p \\left( s_{1} \\right) p_{\\theta} \\left( a_{1} | s_{1} \\right) p \\left( s_{2} | s_{1}, a_{1} \\right) p_{\\theta} \\left( a_{2} | s_{2} \\right) p \\left( s_{3} | s_{2}, a_{2} \\right) \\cdots} } } \\\\ { {} } &amp; { {} { {} { {}=p \\left( s_{1} \\right) \\prod_{t=1}^{T} p_{\\theta} \\left( a_{t} | s_{t} \\right) p \\left( s_{t+1} | s_{t}, a_{t} \\right)} } } \\\\ \\end{aligned} pθ​(τ)​=p(s1​)pθ​(a1​∣s1​)p(s2​∣s1​,a1​)pθ​(a2​∣s2​)p(s3​∣s2​,a2​)⋯=p(s1​)t=1∏T​pθ​(at​∣st​)p(st+1​∣st​,at​)​ 我们能够计算的是RRR(τττ)的期望值。给定某一组参数 θθθ，我们可计算RθR_\\thetaRθ​ 的期望值为 Rˉθ=∑τR(τ)pθ(τ)\\bar{R}_{\\theta}=\\sum_{\\tau} R ( \\tau) p_{\\theta} ( \\tau) Rˉθ​=τ∑​R(τ)pθ​(τ) 因为我们要让奖励越大越好，所以可以使用梯度上升（gradient ascent）来最大化期望奖励。 ∇Rˉθ=∑τR(τ)∇pθ(τ)=∑τR(τ)pθ(τ)∇pθ(τ)pθ(τ)=∑τR(τ)pθ(τ)∇log⁡pθ(τ)=Eτ∼pθ(τ)[R(τ)∇log⁡pθ(τ)]\\begin{aligned} { abla\\bar{R}_{\\theta} } &amp; { {} { {}=\\sum_{\\tau} R ( \\tau) abla p_{\\theta} ( \\tau)} } \\\\ {} &amp; { {} { {}=\\sum_{\\tau} R ( \\tau) p_{\\theta} ( \\tau) \\frac{ abla p_{\\theta} ( \\tau)} {p_{\\theta} ( \\tau)} } } \\\\ { {} } &amp; { {} { {}=\\sum_{\\tau} R ( \\tau) p_{\\theta} ( \\tau) abla\\operatorname{l o g} p_{\\theta} ( \\tau)} } \\\\ {} &amp; { {} { {}=\\mathbb{E}_{\\tau\\sim p_{\\theta} ( \\tau)} \\left[ R ( \\tau) abla\\operatorname{l o g} p_{\\theta} ( \\tau) \\right] } } \\\\ \\end{aligned} ∇Rˉθ​​=τ∑​R(τ)∇pθ​(τ)=τ∑​R(τ)pθ​(τ)pθ​(τ)∇pθ​(τ)​=τ∑​R(τ)pθ​(τ)∇logpθ​(τ)=Eτ∼pθ​(τ)​[R(τ)∇logpθ​(τ)]​ 实际上，R(τ)R(\\tau)R(τ) 难以计算，所以我们用采样的方式采样 NNN个 τττ 并计算每一个的值，把每一个的值加起来，就可以得到梯度，即： Eτ∼pθ(τ)[R(τ)∇log⁡pθ(τ)]≈1N∑n=1NR(τn)∇log⁡pθ(τn)=1N∑n=1N∑t=1TnR(τn)∇log⁡pθ(atn∣stn)\\begin{aligned} { {\\mathbb{E}_{\\tau\\sim p \\theta( \\tau)} \\left[ R ( \\tau) abla\\operatorname{log} p_{\\theta} ( \\tau) \\right]} } &amp; { {} {} { {} \\approx\\frac{1} {N} \\sum_{n=1}^{N} R \\left( \\tau^{n} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( \\tau^{n} \\right)} } \\\\ { {} } &amp; { {} { {} {}=\\frac{1} {N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n} } R \\left( \\tau^{n} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} \\mid s_{t}^{n} \\right)} } \\\\ \\end{aligned} Eτ∼pθ(τ)​[R(τ)∇logpθ​(τ)]​≈N1​n=1∑N​R(τn)∇logpθ​(τn)=N1​n=1∑N​t=1∑Tn​​R(τn)∇logpθ​(atn​∣stn​)​ 我们计算出梯度后，就可以更新模型： ∇Rˉθ=1N∑n=1N∑t=1TnR(τn)∇log⁡pθ(atn∣stn) abla\\bar{R}_{\\theta}=\\frac{1} {N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R \\left( \\tau^{n} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} | s_{t}^{n} \\right) ∇Rˉθ​=N1​n=1∑N​t=1∑Tn​​R(τn)∇logpθ​(atn​∣stn​) 2.技巧 技巧1. 添加基线 在有些环境中，奖励可能都是正的，学习时需要把所有动作概率都提高。但动作概率和是等于1的，所以，最终只会将奖励高的动作概率提高，而奖励低的动作概率下降。 若此时有一个好的动作没有被采样，而其他正奖励的动作采样了，更新后会把没有采样的动作概率调低，但并不能说明这个动作是一个不好的动作，他只是运气不好没有被采样而已。 为了解决奖励总是正的的问题，我们可以把奖励减 bbb，即: ∇Rˉθ≈1N∑n=1N∑t=1Tn(R(τn)−b)∇log⁡pθ(atn∣stn) abla\\bar{R}_{\\theta} \\approx\\frac{1} {N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} \\left( R \\left( \\tau^{n} \\right)-b \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} \\mid s_{t}^{n} \\right) ∇Rˉθ​≈N1​n=1∑N​t=1∑Tn​​(R(τn)−b)∇logpθ​(atn​∣stn​) 技巧2. 分配合适的分数 一个轨迹的奖励高，并不能说明整个轨迹的所有动作都是好的。如此样例，虽然R1R_1R1​的奖励很高，但很明显，a2a_2a2​ 和 a3a_3a3​ 并不一定是一个好的动作。 当样本量足够多时，可以通过其他轨迹惩罚a2a_2a2​ 和 a3a_3a3​ 解决该问题。 更进一步，我们可以把未来的奖励做一个折扣，即： ∇Rˉθ≈1N∑n=1N∑t=1Tn(∑t′=tTnγt′−trt′n−b)∇log⁡pθ(atn∣stn) abla\\bar{R}_{\\theta} \\approx\\frac{1} {N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} \\left( \\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}-b \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} \\mid s_{t}^{n} \\right) ∇Rˉθ​≈N1​n=1∑N​t=1∑Tn​​(t′=t∑Tn​​γt′−trt′n​−b)∇logpθ​(atn​∣stn​) 因为虽然在某一时刻，执行某一个动作，会影响接下来所有的结果（有可能在某一时刻执行的动作，接下来得到的奖励都是这个动作的功劳），但在一般的情况下，时间拖得越长，该动作的影响力就越小。 3. 小结 实际上就是这么实现的。bbb 可以是依赖状态（state-dependent）的，事实上 bbb 通常是一个网络估计出来的，它是一个网络的输出。我们把 R−bR−bR−b称为优势函数，用Aθ(st,at)A_\\theta(s_t,a_t)Aθ​(st​,at​)来代表优势函数。 优势函数取决于 sss 和 aaa，我们就是要计算在某个状态sss采取某个动作 aaa 的时候，优势函数的值。在计算优势函数值时，我们要计算 ∑t′=tTnrt′n\\textstyle\\sum_{t^{\\prime}=t}^{T_{n}} r_{t^{\\prime}}^{n}∑t′=tTn​​rt′n​，需要有一个模型与环境交互，才能知道接下来得到的奖励。优势函数 Aθ(st,at)A_\\theta(s_t,a_t)Aθ​(st​,at​)的上标是 θθθ，θθθ 代表用模型 θθθ 与环境交互。从时刻 ttt 开始到游戏结束为止，所有 rrr 的加和减去 bbb，这就是优势函数。 优势函数的意义是，假设我们在某一个状态sts_tst​ 执行某一个动作ata_tat​，相较于其他可能的动作，ata_tat​有多好。优势函数在意的不是绝对的好，而是相对的好，即相对优势（relative advantage）。因为在优势函数中，我们会减去一个基线bbb，所以这个动作是相对的好，不是绝对的好。 Aθ(st,at)A_\\theta(s_t,a_t)Aθ​(st​,at​)通常可以由一个网络估计出来，这个网络称为评论员（critic）。 PPO算法 重要性采样 回忆一下策略梯度，根据一个策略采样出一批样本后，这批样本只能使用一次。更新策略过后，需要使用新策略采集新的样本，导致样本的使用效率不高。因此，提出了一个异策略方案。 已知有一个函数f(x)f(x)f(x)，假设不能从分布P(x)P(x)P(x)采样，只能从分布q(x)q(x)q(x)采样，根据q(x)q(x)q(x)采样的样本可以估算出p(x)p(x)p(x)分布的均值。 Ex∼p[f(x)]=∫f(x)p(x)dx=∫f(x)p(x)q(x)q(x)dx=Ex∼q[f(x)p(x)q(x)]\\mathbb{E}_{x \\sim p} [ f ( x ) ] = \\int f ( x ) p ( x ) \\mathrm{d} x=\\int f ( x ) \\frac{p ( x )} {q ( x )} q ( x ) \\mathrm{d} x=\\mathbb{E}_{x \\sim q} [ f ( x ) \\frac{p ( x )} {q ( x )} ] Ex∼p​[f(x)]=∫f(x)p(x)dx=∫f(x)q(x)p(x)​q(x)dx=Ex∼q​[f(x)q(x)p(x)​] 因为是从 qqq 采样数据，所以我们从 qqq 采样出来的每一笔数据，都需要乘一个重要性权重（importance weight）p(x)q(x)\\frac{p ( x )} {q ( x )}q(x)p(x)​来修正这两个分布的差异。 如果采样的点不多，可能会导致均值有较大差异（如图，是否采集最左边那个样本，会对均值的正负性有很大影响） 两个随机变量的平均值相同，并不代表它们的方差相同。 Var⁡x∼p[f(x)]=Ex∼p[f(x)2]−(Ex∼p[f(x)])2\\operatorname{V a r}_{x \\sim p} [ f ( x ) ]=\\mathbb{E}_{x \\sim p} \\left[ f ( x )^{2} \\right]-\\left( \\mathbb{E}_{x \\sim p} [ f ( x ) ] \\right)^{2} Varx∼p​[f(x)]=Ex∼p​[f(x)2]−(Ex∼p​[f(x)])2 Varx∼q[f(x)p(x)q(x)]=Ex∼q[(f(x)p(x)q(x))2]−(Ex∼q[f(x)p(x)q(x)])2=Ex∼p[f(x)2p(x)q(x)]−(Ex∼p[f(x)])2\\begin{aligned} { {\\mathrm{V a r}_{x \\sim q} \\left[ f ( x ) {\\frac{p ( x )} {q ( x )} } \\right]} } &amp; { {} { {} {}=\\mathbb{E}_{x \\sim q} \\left[ \\left( f ( x ) {\\frac{p ( x )} {q ( x )} } \\right)^{2} \\right]-\\left( \\mathbb{E}_{x \\sim q} \\left[ f ( x ) {\\frac{p ( x )} {q ( x )} } \\right] \\right)^{2} } } \\\\ { {} } &amp; { {} { {} {}=\\mathbb{E}_{x \\sim p} \\left[ f ( x )^{2} {\\frac{p ( x )} {q ( x )} } \\right]-\\left( \\mathbb{E}_{x \\sim p} [ f ( x ) ] \\right)^{2} } } \\\\ \\end{aligned} Varx∼q​[f(x)q(x)p(x)​]​=Ex∼q​[(f(x)q(x)p(x)​)2]−(Ex∼q​[f(x)q(x)p(x)​])2=Ex∼p​[f(x)2q(x)p(x)​]−(Ex∼p​[f(x)])2​ 重要性采样对策略梯度算法的优化 这样，我们可以采用πθ′\\pi_{\\theta&#x27;}πθ′​策略来采集样本，用来更新策略πθ\\pi_\\thetaπθ​ ，回报均值的梯度为： ∇Rˉθ=Eτ∼pθ′[pθ(τ)pθ′(τ)R(τ)∇log⁡pθ(τ)] abla\\bar{R}_{\\theta}=\\mathbb{E}_{\\tau\\sim p_{\\theta} \\prime} \\left[ \\frac{p_{\\theta} ( \\tau)} {p_{\\theta^{\\prime}} ( \\tau)} R ( \\tau) abla\\operatorname{l o g} p_{\\theta} ( \\tau) \\right] ∇Rˉθ​=Eτ∼pθ​′​[pθ′​(τ)pθ​(τ)​R(τ)∇logpθ​(τ)] 实际在做策略梯度的时候，我们并不是给整个轨迹 τ\\tauτ 一样的分数，而是将每一个状态-动作对分开计算。实际更新梯度的过程可写为： E(st,at)∼πθ[Aθ(st,at)∇log⁡pθ(atn∣stn)]\\mathbb{E}_{( s_t, a_{t} ) \\sim\\pi\\theta} \\left[ A^{\\theta} \\left( s_{t}, a_{t} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} | s_{t}^{n} \\right) \\right] E(st​,at​)∼πθ​[Aθ(st​,at​)∇logpθ​(atn​∣stn​)] 采用异策略，增加一个修正项： E(st,at)∼πθ′[pθ(st,at)pθ′(st,at)Aθ(st,at)∇log⁡pθ(atn∣stn)]\\mathbb{E}_{( s_{t}, a_{t} ) \\sim\\pi\\theta^{\\prime}} \\left[ \\frac{p_{\\theta} \\left( s_{t}, a_{t} \\right)} {p_{\\theta^{\\prime} } \\left( s_{t}, a_{t} \\right)} A^{\\theta} \\left( s_{t}, a_{t} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} | s_{t}^{n} \\right) \\right] E(st​,at​)∼πθ′​[pθ′​(st​,at​)pθ​(st​,at​)​Aθ(st​,at​)∇logpθ​(atn​∣stn​)] 将Pθ(st,at)P_\\theta(s_t,a_t)Pθ​(st​,at​)和pθ′(st,at)p_{\\theta&#x27;}(s_t,a_t)pθ′​(st​,at​)拆解： pθ(st,at)=pθ(at∣st)pθ(st)pθ′(st,at)=pθ′(at∣st)pθ′(st)\\begin{array} {c} { {p_{\\theta} \\left( s_{t}, a_{t} \\right)=p_{\\theta} \\left( a_{t} | s_{t} \\right) p_{\\theta} ( s_{t} )} } \\\\ { {p_{\\theta^{\\prime} } \\left( s_{t}, a_{t} \\right)=p_{\\theta^{\\prime} } \\left( a_{t} | s_{t} \\right) p_{\\theta^{\\prime} } ( s_{t} )} } \\\\ \\end{array} pθ​(st​,at​)=pθ​(at​∣st​)pθ​(st​)pθ′​(st​,at​)=pθ′​(at​∣st​)pθ′​(st​)​ 假设pθ(st)p_\\theta(s_t)pθ​(st​) 和 pθ′(st)p_{\\theta&#x27;}(s_t)pθ′​(st​) 一样，假设A(st,at)A(s_t,a_t)A(st​,at​) 和 A′(st,at)A&#x27;(s_t,a_t)A′(st​,at​)一样。（我真的很迷） E(st,at)∼πθ′[pθ(at∣st)pθ′(at∣st)Aθ′(st,at)∇log⁡pθ(atn∣stn)]\\mathbb{E}_{( s_{t}, a_{t} ) \\sim\\pi\\theta^{\\prime} } \\left[ \\frac{p_{\\theta} \\left( a_{t} | s_{t} \\right)} {p_{\\theta^{\\prime}} \\left( a_{t} | s_{t} \\right)} A^{\\theta^{\\prime} } \\left( s_{t}, a_{t} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} | s_{t}^{n} \\right) \\right] E(st​,at​)∼πθ′​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)∇logpθ​(atn​∣stn​)] $ \\frac{p_{\\theta} ( a_{t} | s_{t} )}{p_{ \\theta’}(a_t | s_t) }$ 很好算，我们有参数θ\\thetaθ ，它就是一个策略网络。我们输入状态 sts_tst​ 到策略网络中，它会输出每一个ata_tat​的概率。 所以实际上，当我们使用重要性采样的时候，要去优化的目标函数为： Jθ′(θ)=E(st,at)∼πθ′[pθ(at∣st)pθ′(at∣st)Aθ′(st,at)]J^{\\theta^{\\prime} } ( \\theta)=\\mathbb{E}_{( s_{t}, a_{t} ) \\sim\\pi\\theta^{\\prime} } \\left[ \\frac{p_{\\theta} \\left( a_{t} | s_{t} \\right)} {p_{\\theta^{\\prime} } \\left( a_{t} | s_{t} \\right)} A^{\\theta^{\\prime} } \\left( s_{t}, a_{t} \\right) \\right] Jθ′(θ)=E(st​,at​)∼πθ′​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)] 近端策略优化 PPO算法： JPPOθ′(θ)=Jθ′(θ)−βKL(θ,θ′)Jθ′(θ)=E(st,at)∼πθ′[pθ(at∣st)pθ′(at∣st)Aθ′(st,at)]\\begin{aligned} { {} } &amp; { {} { {} J_{\\mathrm{P P O} }^{\\theta^{\\prime} } ( \\theta)=J^{\\theta^{\\prime} } ( \\theta)-\\beta\\mathrm{K L} \\left( \\theta, \\theta^{\\prime} \\right)} } \\\\ { {} } &amp; { {} { {} J^{\\theta^{\\prime} } ( \\theta)=\\mathbb{E}_{( s_{t}, a_{t} ) \\sim\\pi\\theta^{\\prime}} \\left[ \\frac{p_{\\theta} \\left( a_{t} \\mid s_{t} \\right)} {p_{\\theta^{\\prime} } \\left( a_{t} \\mid s_{t} \\right)} A^{\\theta^{\\prime} } \\left( s_{t}, a_{t} \\right) \\right]} } \\\\ \\end{aligned} ​JPPOθ′​(θ)=Jθ′(θ)−βKL(θ,θ′)Jθ′(θ)=E(st​,at​)∼πθ′​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)]​ TRPO算法： JTRPOθ′(θ)=E(st,at)∼πθ′[pθ(at∣st)pθ′(at∣st)Aθ′(st,at)],KL(θ,θ′)&lt;δJ_{\\mathrm{T R P O}}^{\\theta^{\\prime} } ( \\theta)=\\mathbb{E}_{( s_{t}, a_{t} ) \\sim\\pi\\theta^{\\prime} } \\left[ \\frac{p_{\\theta} \\left( a_{t} | s_{t} \\right)} {p_{\\theta^{\\prime} } \\left( a_{t} | s_{t} \\right)} A^{\\theta^{\\prime} } \\left( s_{t}, a_{t} \\right) \\right], \\mathrm{K L} \\left( \\theta, \\theta^{\\prime} \\right) &lt; \\delta JTRPOθ′​(θ)=E(st​,at​)∼πθ′​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)],KL(θ,θ′)&lt;δ Q-learning","tags":["rl"],"categories":["rl"]},{"title":"基础知识","path":"/2024/12/16/RL/baserl/","content":"有偏估计和无偏估计 有偏估计（biased estimate）是指由样本值求得的估计值与待估参数的真值之间有系统误差，其期望值不是待估参数的真值。 机器学习概率统计知识(1): 无偏估计与有偏估计 简述有偏估计和无偏估计","tags":["rl"],"categories":["rl"]},{"title":"mathmatical-rl","path":"/2024/12/15/RL/mathmatical-rl/","content":"第三章、贝尔曼最优方程 Definition 3.1 (Optimal policy and optimal state value). A policy $ \\pi^{ * } $ is optimal if vπ∗(s)≥vπ(s)v_{\\pi^{*} } ( s ) \\geq v_{\\pi} ( s )vπ∗​(s)≥vπ​(s) for all s∈Ss \\in\\mathcal{S}s∈S and for any other policy π\\piπ . The state values of π∗\\pi^{*}π∗ are the optimal state values. 第四章、值迭代和策略迭代 Value Iteration Policy update πk+1(s)=arg⁡max⁡π∑aπ(a∣s)(∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)vk(s′))⏟qk(s,a),s∈S.\\pi_{k+1} ( s )=\\operatorname{a r g} \\operatorname* {m a x}_{\\pi} \\sum_{a} \\pi( a | s ) \\underbrace{\\left( \\sum_{r} p ( r | s, a ) r+\\gamma\\sum_{s^{\\prime} } p ( s^{\\prime} | s, a ) v_{k} ( s^{\\prime} ) \\right)}_{q_{k} ( s, a )}, \\quad s \\in{\\mathcal{S} }. πk+1​(s)=argπmax​a∑​π(a∣s)qk​(s,a)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vk​(s′))​​,s∈S. 采用贪心的策略，该状态下，策略选择的动作唯一。倘若最优策略不唯一，任选其中的一个策略不影响算法的收敛性。 πk+1(a∣s)={1,a=ak∗(s),0,a≠ak∗(s),\\pi_{k+1} ( a | s )=\\left\\{\\begin{array} {c c} { {1,} } &amp; { {a=a_{k}^{*} ( s ),} } \\\\ { {0,} } &amp; { {a eq a_{k}^{*} ( s ),} } \\\\ \\end{array} \\right. πk+1​(a∣s)={1,0,​a=ak∗​(s),a=ak∗​(s),​ Value update vk+1(s)=∑aπk+1(a∣s)(∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)vk(s′))⏟qk(s,a),s∈S.v_{k+1} ( s )=\\sum_{a} \\pi_{k+1} ( a | s ) \\underbrace{\\left( \\sum_{r} p ( r | s, a ) r+\\gamma\\sum_{s^{\\prime} } p ( s^{\\prime} | s, a ) v_{k} ( s^{\\prime} ) \\right)}_{q_{k} ( s, a )}, \\quad s \\in\\mathcal{S}. vk+1​(s)=a∑​πk+1​(a∣s)qk​(s,a)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vk​(s′))​​,s∈S. vk+1(s)=max⁡aqk(s,a).v_{k+1} ( s )=\\operatorname* {m a x}_{a} q_{k} ( s, a ). vk+1​(s)=amax​qk​(s,a). 迭代过程： vk(s)→qk(s,a)→new greedy policy πk+1(s)→new value vk+1(s)=max⁡aqk(s,a)v_k(s) → q_k(s,a) → new \\; greedy \\; policy \\; \\pi_{k+1} ( s ) \\to new \\; value \\; v_{k+1} ( s ) = \\operatorname* {m a x}_{a} q_{k} ( s, a ) vk​(s)→qk​(s,a)→newgreedypolicyπk+1​(s)→newvaluevk+1​(s)=amax​qk​(s,a) 代码 1234567891011121314def value_iteration(env, gamma=1.0): v = np.zeros(env.env.observation_space.n) # initialize value-function max_iterations = 100000 eps = 1e-20 for i in range(max_iterations): prev_v = np.copy(v) for s in range(env.env.observation_space.n): # 在s状态下，采取a动作，转移到s_，概率为p，回报为r。 q_sa = [sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.env.action_space.n)] v[s] = max(q_sa) if (np.sum(np.fabs(prev_v - v)) &lt;= eps): print(&#x27;Value-iteration converged at iteration# %d.&#x27; % (i + 1)) break return v Policy Iteration policy evaluation vπk=rπk+γPπkvπkv_{\\pi_{k}}=r_{\\pi_{k}}+\\gamma P_{\\pi_{k}} v_{\\pi_{k}} vπk​​=rπk​​+γPπk​​vπk​​ policy improvement πk+1=arg⁡max⁡π(rπ+γPπvπk)\\pi_{k+1}=\\operatorname{a r g} \\operatorname* {m a x}_{\\pi} ( r_{\\pi}+\\gamma P_{\\pi} v_{\\pi_{k}} ) πk+1​=argπmax​(rπ​+γPπ​vπk​​) 代码 12345678910111213def policy_iteration(env, gamma=1.0): &quot;&quot;&quot; Policy-Iteration algorithm &quot;&quot;&quot; policy = np.random.choice(env.env.action_space.n, size=(env.env.observation_space.n)) # initialize a random policy max_iterations = 200000 gamma = 1.0 for i in range(max_iterations): old_policy_v = compute_policy_v(env, policy, gamma) new_policy = extract_policy(old_policy_v, gamma) if (np.all(policy == new_policy)): print(&#x27;Policy-Iteration converged at step %d.&#x27; % (i + 1)) break policy = new_policy return policy Summary Value Iteration 只对价值做一次计算 Plolicy Iteration 对价值做计算至收敛（无穷次计算） 因此提出了截断式策略迭代（对价值进行有限次计算） Policy Iteration : π0⟶PEvπ0⟶PIπ1⟶PEvπ1⟶PIπ2⟶PEvπ2⟶PI…\\pi_{0} \\stackrel{P E} {\\longrightarrow} v_{\\pi_{0}} \\stackrel{P I} {\\longrightarrow} \\pi_{1} \\stackrel{P E} {\\longrightarrow} v_{\\pi_{1}} \\stackrel{P I} {\\longrightarrow} \\pi_{2} \\stackrel{P E} {\\longrightarrow} v_{\\pi_{2}} \\stackrel{P I} {\\longrightarrow} \\ldotsπ0​⟶PE​vπ0​​⟶PI​π1​⟶PE​vπ1​​⟶PI​π2​⟶PE​vπ2​​⟶PI​… Value Iteration : v0→PUπ1′→VUv1→PUπ2′→VUv2→PU…v_{0} \\xrightarrow{P U} \\pi_{1}^{\\prime} \\xrightarrow{V U} v_{1} \\xrightarrow{P U} \\pi_{2}^{\\prime} \\xrightarrow{V U} v_{2} \\xrightarrow{P U} \\ldotsv0​PU​π1′​VU​v1​PU​π2′​VU​v2​PU​… 第六章、随机近似 平均值怎么计算？ 直接计算 E[X]≈xˉ≐1n∑i=1nxi.\\mathbb{E} [ X ] \\approx\\bar{x} \\doteq\\frac{1} {n} \\sum_{i=1}^{n} x_{i}. E[X]≈xˉ≐n1​i=1∑n​xi​. incremental manner (递增方式) wk+1=1k∑i=1kxi=1k(∑i=1k−1xi+xk)=1k((k−1)wk+xk)=wk−1k(wk−xk).w_{k+1}=\\frac{1} {k} \\sum_{i=1}^{k} x_{i}=\\frac{1} {k} \\left( \\sum_{i=1}^{k-1} x_{i}+x_{k} \\right)=\\frac{1} {k} ( ( k-1 ) w_{k}+x_{k} )=w_{k}-\\frac{1} {k} ( w_{k}-x_{k} ). wk+1​=k1​i=1∑k​xi​=k1​(i=1∑k−1​xi​+xk​)=k1​((k−1)wk​+xk​)=wk​−k1​(wk​−xk​). 结果： w1=x1,w2=w1−11(w1−x1)=x1,w3=w2−12(w2−x2)=x1−12(x1−x2)=12(x1+x2),w4=w3−13(w3−x3)=13(x1+x2+x3),\\begin{aligned} { {} } &amp; { {} { {} w_{1}=x_{1},} } \\\\ { {} } &amp; { {} { {} w_{2}=w_{1}-\\frac{1} {1} ( w_{1}-x_{1} )=x_{1},} } \\\\ { {} } &amp; { {} { {} w_{3}=w_{2}-\\frac{1} {2} ( w_{2}-x_{2} )=x_{1}-\\frac{1} {2} ( x_{1}-x_{2} )=\\frac{1} {2} ( x_{1}+x_{2} ),} } \\\\ { {} } &amp; { {} { {} w_{4}=w_{3}-\\frac{1} {3} ( w_{3}-x_{3} )=\\frac{1} {3} ( x_{1}+x_{2}+x_{3} ),} } \\\\ \\end{aligned} ​w1​=x1​,w2​=w1​−11​(w1​−x1​)=x1​,w3​=w2​−21​(w2​−x2​)=x1​−21​(x1​−x2​)=21​(x1​+x2​),w4​=w3​−31​(w3​−x3​)=31​(x1​+x2​+x3​),​ wk+1=1k∑i=1kxi.w_{k+1}=\\frac{1} {k} \\sum_{i=1}^{k} x_{i}. wk+1​=k1​i=1∑k​xi​. 优势：求第kkk步不需要再将之前的值加起来，只需要一步计算即可得到新一次的平均数。 Robbins-Monro algorithm Suppose that we would like to find the root of the equation: g(w)=0g(w) = 0 g(w)=0 For example, if J(w)J(w)J(w) is an objective function to be optimized, this optimization problem can be converted to solving: g(w)≐∇wJ(w)=0.g ( w ) \\doteq abla_{w} J ( w )=0. g(w)≐∇w​J(w)=0. The function may be represented by an artificial neural network whose structure and parameters are unknown. Moreover, we can only obtain a noisy observation of g(w): g~(w,η)=g(w)+η,\\tilde{g} ( w, \\eta)=g ( w )+\\eta, g~​(w,η)=g(w)+η, **Our aim is to solve g(w)g(w)g(w) = 0 using www and g~\\widetilde{g}g​ ** . The RM algorithm that can solve g(w)=0g(w)= 0g(w)=0 is: wk+1=wk−akg~(wk,ηk),k=1,2,3,…w_{k+1}=w_{k}-a_{k} \\tilde{g} ( w_{k}, \\eta_{k} ), \\qquad k=1, 2, 3, \\ldots wk+1​=wk​−ak​g~​(wk​,ηk​),k=1,2,3,… Theorem (Robbins-Monro theorem). In the Robbins-Monro algorithm in (6.5), if (a) 0&lt;c1≤∇wg(w)≤c20 &lt; c_{1} \\leq abla_{w} g ( w ) \\leq c_{2}0&lt;c1​≤∇w​g(w)≤c2​ for all www ; (b） ∑k=1∞ak=∞\\sum_{k=1}^{\\infty} a_{k}=\\infty∑k=1∞​ak​=∞ and ∑k=1∞ak2&lt;∞\\sum_{k=1}^{\\infty} a_{k}^{2} &lt; \\infty∑k=1∞​ak2​&lt;∞ ; (c） E[ηk∣Hk]=0\\mathbb{E} [ \\eta_{k} | {\\mathcal{H}}_{k} ]=0E[ηk​∣Hk​]=0 and E[ηk2∣Hk]&lt;∞\\mathbb{E} [ \\eta_{k}^{2} | {\\mathcal{H}}_{k} ] &lt; \\inftyE[ηk2​∣Hk​]&lt;∞ ; where Hk={wk,wk−1,…}{\\mathcal H}_{k}=\\{w_{k}, w_{k-1}, \\ldots\\}Hk​={wk​,wk−1​,…} , then wkw_{k}wk​ almost surely converges to the root w∗w^{*}w∗ satisfying g(w∗)=0g ( w^{*} )=0g(w∗)=0","tags":["rl"],"categories":["rl"]},{"title":"python","path":"/2024/12/15/RL/python/","content":"pytorch基础 Tensor（张量） 维度（Dimensionality）：数据的多维数组结构。 形状（Shape）：张量的形状是指每个维度上的大小。 数据类型（Dtype）：如（torch.int8、torch.int32）、浮点型（如torch.float32、torch.float64）和布尔型（torch.bool）。 torch中函数 torch.view() 定义：调整Tensor的形状 12345678import torchtensor_0 = torch.arange(1, 13)tensor_1 = tensor_0.view(3, 4)tensor_2 = tensor_0.view(-1,6)print(tensor_1)print(tensor_2) 12345tensor([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])tensor([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12]]) torch.gather() 定义：从原tensor中获取指定dim和指定index的数据 用途：方便从批量tensor中获取指定索引下的数据，该索引是高度自定义化的，可乱序的 1torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor 123out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 参考链接：图解PyTorch中的torch.gather函数","tags":["rl"],"categories":["rl"]},{"title":"强化学习算法","path":"/2024/12/15/RL/handsRL/","content":"DQN 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class DQN: &#x27;&#x27;&#x27; DQN算法,包括Double DQN和Dueling DQN &#x27;&#x27;&#x27; def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device, dqn_type=&#x27;VanillaDQN&#x27;): self.action_dim = action_dim if dqn_type == &#x27;DuelingDQN&#x27;: # Dueling DQN采取不一样的网络框架 self.q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device) self.target_q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device) else: self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate) self.gamma = gamma self.epsilon = epsilon self.target_update = target_update self.count = 0 self.dqn_type = dqn_type self.device = device def take_action(self, state): if np.random.random() &lt; self.epsilon: action = np.random.randint(self.action_dim) else: state = torch.tensor([state], dtype=torch.float).to(self.device) action = self.q_net(state).argmax().item() return action def max_q_value(self, state): state = torch.tensor([state], dtype=torch.float).to(self.device) return self.q_net(state).max().item() def update(self, transition_dict): states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[&#x27;actions&#x27;]).view(-1, 1).to( self.device) rewards = torch.tensor(transition_dict[&#x27;rewards&#x27;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[&#x27;next_states&#x27;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[&#x27;dones&#x27;], dtype=torch.float).view(-1, 1).to(self.device) q_values = self.q_net(states).gather(1, actions) if self.dqn_type == &#x27;DoubleDQN&#x27;: max_action = self.q_net(next_states).max(1)[1].view(-1, 1) max_next_q_values = self.target_q_net(next_states).gather( 1, max_action) else: max_next_q_values = self.target_q_net(next_states).max(1)[0].view( -1, 1) q_targets = rewards + self.gamma * max_next_q_values * (1 - dones) dqn_loss = torch.mean(F.mse_loss(q_values, q_targets)) self.optimizer.zero_grad() dqn_loss.backward() self.optimizer.step() if self.count % self.target_update == 0: self.target_q_net.load_state_dict(self.q_net.state_dict()) self.count += 1 DQN与Double DQN区别 高估问题：DQN 存在一个问题，即它倾向于高估 Q 值。这是因为在计算目标 Q 值时，是通过选择下一个状态中 Q 值最大的动作来计算的。由于 Q 值是估计值，存在误差，这种选择最大值的方式会导致高估。 Double DQN 的改进：Double DQN 的主要改进在于它将动作选择和动作评估进行了分离，从而减少 Q 值的高估。在 Double DQN 中，选择下一个动作时使用当前的 Q 网络，而评估这个动作的 Q 值时使用目标 Q 网络。 123456# 下个状态的最大Q值if self.dqn_type == &#x27;DoubleDQN&#x27;: # DQN与Double DQN的区别 max_action = self.q_net(next_states).max(1)[1].view(-1, 1) max_next_q_values = self.target_q_net(next_states).gather(1, max_action)else: # DQN的情况 max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1) Dueling DQN 它的核心思想是将 Q - 网络的输出分为两个部分：状态价值函数VVV和优势函数AAA。QQQ 值函数可以表示为：Q(s,a)=V(s)+A(s,a)Q(s,a) = V(s) + A(s,a)Q(s,a)=V(s)+A(s,a) 状态价值函数V(s)V(s)V(s)表示在状态下的价值，而优势函数A(s,a)A(s,a)A(s,a)表示采取动作相对于平均动作价值在状态下的优势。这种分解的好处是可以更灵活地学习状态价值和动作优势，尤其在某些状态下，动作的影响可能比状态本身的价值变化小，Dueling DQN 能够更好地捕捉这种特性。 12345678910111213class VAnet(torch.nn.Module): &#x27;&#x27;&#x27; 只有一层隐藏层的A网络和V网络 &#x27;&#x27;&#x27; def __init__(self, state_dim, hidden_dim, action_dim): super(VAnet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) # 共享网络部分 self.fc_A = torch.nn.Linear(hidden_dim, action_dim) self.fc_V = torch.nn.Linear(hidden_dim, 1) def forward(self, x): A = self.fc_A(F.relu(self.fc1(x))) V = self.fc_V(F.relu(self.fc1(x))) Q = V + A - A.mean(1).view(-1, 1) # Q值由V值和A值计算得到 return Q 策略梯度算法 假设目标策略πθ\\pi_\\thetaπθ​是一个随机性策略，并且处处可微，其中θ\\thetaθ是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。 J(θ)=Es0[Vπθ(s0)]J(\\theta)=\\mathbb{E}_{s_{0}}\\left[V^{\\pi_{\\theta}}\\left(s_{0}\\right)\\right] J(θ)=Es0​​[Vπθ​(s0​)] 其中，s0s_0s0​表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。 ∇θJ(θ)∝∑s∈Sνπθ(s)∑a∈AQπθ(s,a)∇θπθ(a∣s)=∑s∈Sνπθ(s)∑a∈Aπθ(a∣s)Qπθ(s,a)∇θπθ(a∣s)πθ(a∣s)=Eπθ[Qπθ(s,a)∇θlog⁡πθ(a∣s)]\\begin{aligned} abla_{\\theta} J(\\theta) &amp; \\propto \\sum_{s \\in S} u^{\\pi_{\\theta}}(s) \\sum_{a \\in A} Q^{\\pi_{\\theta}}(s, a) abla_{\\theta} \\pi_{\\theta}(a \\mid s) \\\\ &amp; =\\sum_{s \\in S} u^{\\pi_{\\theta}}(s) \\sum_{a \\in A} \\pi_{\\theta}(a \\mid s) Q^{\\pi_{\\theta}}(s, a) \\frac{ abla_{\\theta} \\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta}(a \\mid s)} \\\\ &amp; =\\mathbb{E}_{\\pi_{\\theta}}\\left[Q^{\\pi_{\\theta}}(s, a) abla_{\\theta} \\log \\pi_{\\theta}(a \\mid s)\\right] \\end{aligned} ∇θ​J(θ)​∝s∈S∑​νπθ​(s)a∈A∑​Qπθ​(s,a)∇θ​πθ​(a∣s)=s∈S∑​νπθ​(s)a∈A∑​πθ​(a∣s)Qπθ​(s,a)πθ​(a∣s)∇θ​πθ​(a∣s)​=Eπθ​​[Qπθ​(s,a)∇θ​logπθ​(a∣s)]​ Reinforce算法流程 123456789class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) 12345678910111213141516171819202122232425262728293031323334class REINFORCE: def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device): self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate) # 使用Adam优化器 self.gamma = gamma # 折扣因子 self.device = device def take_action(self, state): # 根据动作概率分布随机采样 state = torch.tensor([state], dtype=torch.float).to(self.device) probs = self.policy_net(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict): reward_list = transition_dict[&#x27;rewards&#x27;] state_list = transition_dict[&#x27;states&#x27;] action_list = transition_dict[&#x27;actions&#x27;] G = 0 self.optimizer.zero_grad() for i in reversed(range(len(reward_list))): # 从最后一步算起 reward = reward_list[i] state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device) action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device) log_prob = torch.log(self.policy_net(state).gather(1, action)) G = self.gamma * G + reward loss = -log_prob * G # 每一步的损失函数 loss.backward() # 反向传播计算梯度 self.optimizer.step() # 梯度下降 Actor-Critic 1.概述 Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。 Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。 2.策略函数更新 拟合一个值函数来指导策略进行学习 g=E[∑t=0Tψt∇θlog⁡πθ(at∣st)]g=\\mathbb{E} \\left[ \\sum_{t=0}^{T} \\psi_{t} abla_{\\theta} \\operatorname{l o g} \\pi_{\\theta} ( a_{t} | s_{t} ) \\right] g=E[t=0∑T​ψt​∇θ​logπθ​(at​∣st​)] ψ\\psiψ可以有多种形式： ∑t′=0Tγt′rt′\\sum_{t^{\\prime}=0}^{T} \\gamma^{t^{\\prime}} r_{t^{\\prime}}∑t′=0T​γt′rt′​ :轨迹的总回报; ∑t′=tTγt′−trt′\\sum_{t^{\\prime}=t}^{T} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}∑t′=tT​γt′−trt′​ ：动作ata_tat​之后的回报； ∑t′=tTγt′−trt′−b(st)\\sum_{t^{\\prime}=t}^{T} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}-b ( s_{t} )∑t′=tT​γt′−trt′​−b(st​) ：基准线版本的改进； Qπθ(st,at)Q^{\\pi_{\\theta}} ( s_{t}, a_{t} )Qπθ​(st​,at​) : 动作价值函数; Aπθ(st,at)A^{\\pi_{\\theta}} ( s_{t}, a_{t} )Aπθ​(st​,at​) ：优势函数； rt+γVπθ(st+1)−Vπθ(st)r_{t}+\\gamma V^{\\pi_{\\theta}} ( s_{t+1} )-V^{\\pi_{\\theta}} ( s_{t} )rt​+γVπθ​(st+1​)−Vπθ​(st​) :时序差分残差。 3. 价值函数更新 我们将 Critic 价值网络表示为VwV_wVw​，参数为$w $。于是，我们可以采取时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数： L(ω)=12(r+γVω(st+1)−Vω(st))2\\mathcal{L} ( \\omega)=\\frac{1} {2} ( r+\\gamma V_{\\omega} ( s_{t+1} )-V_{\\omega} ( s_{t} ) )^{2} L(ω)=21​(r+γVω​(st+1​)−Vω​(st​))2 将上式中r+γVω(st+1)r+\\gamma V_{\\omega} ( s_{t+1} )r+γVω​(st+1​)作为时序差分目标，不会产生梯度来更新价值函数。因此，价值函数的梯度为： ∇ωL(ω)=−(r+γVω(st+1)−Vω(st))∇ωVω(st) abla_{\\omega} \\mathcal{L} ( \\omega)=-( r+\\gamma V_{\\omega} ( s_{t+1} )-V_{\\omega} ( s_{t} ) ) abla_{\\omega} V_{\\omega} ( s_{t} ) ∇ω​L(ω)=−(r+γVω​(st+1​)−Vω​(st​))∇ω​Vω​(st​) 4.算法流程 5.代码 123456789class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) 123456789class ValueNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim): super(ValueNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, 1) def forward(self, x): x = F.relu(self.fc1(x)) return self.fc2(x) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class ActorCritic: def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device): # 策略网络 self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.critic = ValueNet(state_dim, hidden_dim).to(device) # 价值网络 # 策略网络优化器 self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr) self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) # 价值网络优化器 self.gamma = gamma self.device = device def take_action(self, state): state = torch.tensor([state], dtype=torch.float).to(self.device) probs = self.actor(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict): states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[&#x27;actions&#x27;]).view(-1, 1) .to(self.device) rewards = torch.tensor(transition_dict[&#x27;rewards&#x27;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[&#x27;next_states&#x27;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[&#x27;dones&#x27;], dtype=torch.float).view(-1, 1).to(self.device) # 时序差分目标 td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones) td_delta = td_target - self.critic(states) # 时序差分误差 log_probs = torch.log(self.actor(states).gather(1, actions)) actor_loss = torch.mean(-log_probs * td_delta.detach()) # 均方误差损失函数 critic_loss = torch.mean( F.mse_loss(self.critic(states), td_target.detach())) self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() actor_loss.backward() # 计算策略网络的梯度 critic_loss.backward() # 计算价值网络的梯度 self.actor_optimizer.step() # 更新策略网络的参数 self.critic_optimizer.step() # 更新价值网络的参数 Trpo算法 1.概述 TRPO（Trust Region Policy Optimization）算法一种基于策略梯度（Policy Gradient）的强化学习算法。它主要是为了解决策略梯度方法在优化过程中由于步长选择不当而导致的性能下降问题。 TRPO 基于信赖域（Trust Region）的思想，试图在一个 “可信赖” 的区域内对策略进行优化。这个信赖域是通过限制新策略和旧策略之间的差异来定义的，以确保策略的更新不会过于激进，从而保证策略性能的稳定提升。它的核心是在优化目标函数（通常是累计奖励的期望）的同时，满足一个约束条件，这个约束条件与新旧策略之间的 **KL - 散度（Kullback - Leibler Divergence）**有关。KL - 散度用于衡量两个概率分布之间的差异，在这里用于衡量新旧策略之间的距离。 2.基础知识 2.1KL散度 KL散度（Kullback-Leibler divergence），可以以称作相对熵（relative entropy）或信息散度（information divergence）。KL散度的理论意义在于度量两个概率分布之间的差异程度，当KL散度越大的时候，说明两者的差异程度越大；而当KL散度小的时候，则说明两者的差异程度小。如果两者相同的话，则该KL散度应该为0。 离散随机变量，P对Q的散度： DKL(P∣∣Q)=∑iP(i)ln⁡(P(i)Q(i))\\mathbb{D}_{\\mathrm{K L}} ( P | | Q )=\\sum_{i} P ( i ) \\operatorname{l n} ( \\frac{P ( i )} {Q ( i )} ) DKL​(P∣∣Q)=i∑​P(i)ln(Q(i)P(i)​) 连续随机变量，P对Q的散度： DKL(P∣∣Q)=∫−∞∞p(x)ln⁡(p(x)q(x))dx\\mathbb{D}_{\\mathrm{K L}} ( P | | Q )=\\int_{-\\infty}^{\\infty} p ( {\\bf x} ) \\operatorname{l n} ( \\frac{p ( {\\bf x} )} {q ( {\\bf x} )} ) d {\\bf x} DKL​(P∣∣Q)=∫−∞∞​p(x)ln(q(x)p(x)​)dx 参考链接： [1]机器学习_KL散度详解 [2]关于KL散度（Kullback-Leibler Divergence）的笔记 2.2共轭梯度法 参考资料： [1]共轭梯度法简介 PPO算法 以PPO-截断为例，优化目标为： arg⁡max⁡θEs∼νπθkEθ∼πθk(⋅∣s)[min⁡(πθ(a∣s)πθk(a∣s)Aπθk(s,a),clip(πθ(a∣s)πθk(a∣s),1−ϵ,1+ϵ)Aπθk(s,a))]\\operatorname{a r g} \\operatorname* {m a x}_{\\theta} \\mathbb{E}_{s \\sim u} \\pi_{\\theta_{k} } \\mathbb{E}_{\\theta\\sim\\pi_{\\theta_{k} } ( \\cdot| s )} \\left[ \\operatorname* {m i n} \\left( \\frac{\\pi_{\\theta} ( a | s )} {\\pi_{\\theta_{k} } ( a | s )} A^{\\pi_{\\theta_{k} } } ( s, a ), \\mathrm{c l i p} \\left( \\frac{\\pi_{\\theta} ( a | s )} {\\pi_{\\theta_{k} } ( a | s )}, 1-\\epsilon, 1+\\epsilon\\right) A^{\\pi_{\\theta_{k} } } ( s, a ) \\right) \\right] argθmax​Es∼ν​πθk​​Eθ∼πθk​​(⋅∣s)​[min(πθk​​(a∣s)πθ​(a∣s)​Aπθk​​(s,a),clip(πθk​​(a∣s)πθ​(a∣s)​,1−ϵ,1+ϵ)Aπθk​​(s,a))] 代码： 定义网络结构 策略网络(Actor)：给定状态，选取对应的动作。 123456789class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) 价值网络(Critic)：指导策略学习 123456789class ValueNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim): super(ValueNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, 1) def forward(self, x): x = F.relu(self.fc1(x)) return self.fc2(x) 定义PPO算法结构 12345678910111213def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, eps, gamma, device): self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.critic = ValueNet(state_dim, hidden_dim).to(device) self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr) self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) self.gamma = gamma self.lmbda = lmbda self.epochs = epochs # 一条序列的数据用来训练轮数 self.eps = eps # PPO中截断范围的参数 self.device = device 动作选取 根据策略网络选取价值概率最大的动作。 1234567def take_action(self, state): state = torch.tensor([state], dtype=torch.float).to(self.device) probs = self.actor(state) # 相当于构建一个选取动作的概率分布，从中采取一个样本作为动作 action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() 参考链接：pytorch categorical 用法及事例 更新 1234567891011121314151617181920212223242526272829303132def update(self, transition_dict): states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[&#x27;actions&#x27;]).view(-1, 1).to( self.device) rewards = torch.tensor(transition_dict[&#x27;rewards&#x27;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[&#x27;next_states&#x27;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[&#x27;dones&#x27;], dtype=torch.float).view(-1, 1).to(self.device) td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones) td_delta = td_target - self.critic(states) # 时序差分残差 advantage = rl_utils.compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device) old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach() for _ in range(self.epochs): log_probs = torch.log(self.actor(states).gather(1, actions)) ratio = torch.exp(log_probs - old_log_probs) # 调节参数(p(x)/q(x)) surr1 = ratio * advantage surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage # 截断 actor_loss = torch.mean(-torch.min(surr1, surr2)) # PPO损失函数 critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach())) self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() actor_loss.backward() critic_loss.backward() self.actor_optimizer.step() self.critic_optimizer.step() 总结 Q-learning 使用时序查分误差学习Q(s,a)Q(s,a)Q(s,a) Q(s,a)←Q(s,a)+α[r+γmax⁡a′∈AQ(s′,a′)−Q(s,a)]Q ( s, a ) \\gets Q ( s, a )+\\alpha\\left[ r+\\gamma\\operatorname* {m a x}_{a^{\\prime} \\in\\mathcal{A} } Q ( s^{\\prime}, a^{\\prime} )-Q ( s, a ) \\right] Q(s,a)←Q(s,a)+α[r+γa′∈Amax​Q(s′,a′)−Q(s,a)] DQN 使用神经网络Q−NetQ-NetQ−Net拟合动作价值，损失函数定义为： ω∗=arg⁡min⁡ω12N∑i=1N[Qω(si,ai)−(ri+γmax⁡a′Qω(si′,a′))]2\\omega^{*}=\\operatorname{a r g} \\operatorname* {m i n}_{\\omega} \\frac{1} {2 N} \\sum_{i=1}^{N} \\left[ Q_{\\omega} \\left( s_{i}, a_{i} \\right)-\\left( r_{i}+\\gamma\\operatorname* {m a x}_{a^{\\prime} } Q_{\\omega} \\left( s_{i}^{\\prime}, a^{\\prime} \\right) \\right) \\right]^{2} ω∗=argωmin​2N1​i=1∑N​[Qω​(si​,ai​)−(ri​+γa′max​Qω​(si′​,a′))]2 Double DQN Double DQN 的主要改进在于它将动作选择和动作评估进行了分离，从而减少 Q 值的高估。在 Double DQN 中，选择下一个动作时使用当前的 Q 网络，而评估这个动作的 Q 值时使用目标 Q 网络。 Dueling DQN 通过拟合状态价值函数和优势函数，来计算动作价值函数。 策略梯度算法 DQN是基于价值的，而策略梯度算法是基于策略的，学习的是策略网络的参数。 ∇Rˉθ=1N∑n=1N∑t=1TnR(τn)∇log⁡pθ(atn∣stn) abla\\bar{R}_{\\theta}=\\frac{1} {N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R \\left( \\tau^{n} \\right) abla\\operatorname{l o g} p_{\\theta} \\left( a_{t}^{n} | s_{t}^{n} \\right) ∇Rˉθ​=N1​n=1∑N​t=1∑Tn​​R(τn)∇logpθ​(atn​∣stn​) ReInForce算法 基于蒙特卡洛采样，一个trajectory更新一次。 Actor-Critic算法 策略网络选取动作，价值网络评估策略。 策略网络根据时序差分误差更新 价值网络根据新策略网络与旧策略网络的目标函数值的均方误差更新。 Trpo算法 在信任区域，选取尽可能大的步长，使用KL散度约束。 PPO算法 近端策略优化-惩罚：将KL散度作为惩罚项扔到网络里学习 JPPOθk(θ)=Jθk(θ)−βKL(θ,θk)Jθk(θ)≈∑(st,at)pθ(at∣st)pθk(at∣st)Aθk(st,at)\\begin{array} {l} { { { {J_{\\mathrm{P P O} }^{\\theta^{k} } ( \\theta)=J^{\\theta^{k} } ( \\theta)-\\beta\\mathrm{K L} \\left( \\theta, \\theta^{k} \\right)} } } } \\\\ { { { {J^{\\theta^{k} } ( \\theta) \\approx\\sum_{( s_{t}, a_{t} )} \\frac{p_{\\theta} \\left( a_{t} \\mid s_{t} \\right)} {p_{\\theta^{k} } \\left( a_{t} \\mid s_{t} \\right)} A^{\\theta^{k} } ( s_{t}, a_{t} )} } } } \\\\ \\end{array} JPPOθk​(θ)=Jθk(θ)−βKL(θ,θk)Jθk(θ)≈∑(st​,at​)​pθk​(at​∣st​)pθ​(at​∣st​)​Aθk(st​,at​)​ 近端策略优化-截断：限制模型更新前后差距不要太大 L(s,a,θk,θ)=min⁡(πθ(a∣s)πθk(a∣s)Aπθk(s,a), dip⁡(πθ(a∣s)πθk(a∣s),1−ϵ,1+ϵ)Aπθk(s,a)),L ( s, a, \\theta_{k}, \\theta)=\\operatorname* {m i n} \\left( \\frac{\\pi_{\\theta} ( a | s )} {\\pi_{\\theta_{k} } ( a | s )} A^{\\pi_{\\theta_{k} } } ( s, a ), \\ \\ \\operatorname* {d i p} \\left( \\frac{\\pi_{\\theta} ( a | s )} {\\pi_{\\theta_{k} } ( a | s )}, 1-\\epsilon, 1+\\epsilon\\right) A^{\\pi_{\\theta_{k} } } ( s, a ) \\right), L(s,a,θk​,θ)=min(πθk​​(a∣s)πθ​(a∣s)​Aπθk​​(s,a), dip(πθk​​(a∣s)πθ​(a∣s)​,1−ϵ,1+ϵ)Aπθk​​(s,a)),","tags":["rl"],"categories":["rl"]},{"title":"cet-6","path":"/2024/12/14/其他/cet-6/","content":"模板 第一段： 1There is a growing awareness of the importance of __. In the contemporary world , __ have/has been increasingly important. It&#x27;s great neccesity for __ to __. Reasons and concrete evidence to support my viewpoint are as follows. 第二段 12345678In the first place,there is no doubt that __.Based on big data , most of __ admitted that __.Moreover, no one can deny that __.Where there is/are __ , there is/ara __.Last but not least,I firmly believe that __.The more fans you have , the happier you are. 第三段 1In conclusion, __. If we spare no efforts to __ , the furture of __ will be both hopeful and rosy. 范文 12345There is a growing awareness of the importance of digital literacy and skills in today&#x27;s world. In the contemporary world, digital literacy has become increasingly important.It&#x27;s of great necessity for individuals to equip themselves with digital skills to thrive in both personal and professional settings. Reasons and concrete evidence to support my viewpoint are as follows.In the first place,there is no doubt that digital literacy enhances productivity and efficiency.Based on big data,most professionals admitted that they have spent 2/3 of their time utilizing digital tools and platforms to perform their tasks.Moreover,no one can deny that digital literacy opens up numerous opportunities for career advancement.Where there are strong digital skills,there are competitive advantages in the job market.Last but not least,I firmly believe that digital literacy fosters lifelong learning and adaptability.The more digitally literate you are,the more capable you are of embracing new technologies and adapting to changes.In conclusion,digital literacy is essential for success in today&#x27;s world.If we spare no efforts to improve our digital skills,the future of our personal and professional lives will be both hopeful and rosy. There is a growing awareness of the importance of digital literacy and skills in today’s world. It’s of great necessity for individuals to equip themselves with digital skills to thrive in both personal and professional settings. Reasons and concrete evidence to support my viewpoint are as follows. In the first place, there is no doubt that digital literacy enhances productivity and efficiency. Based on big data, most professionals admitted that they have spent 2/3 of their time utilizing digital tools and platforms to perform their tasks. Moreover, no one can deny that digital literacy opens up numerous opportunities for career advancement. Where there are strong digital skills,there are competitive advantages in the job market. Last but not least, I firmly believe that digital literacy fosters lifelong learning and adaptability. The more digitally literate you are, the more capable you are of embracing new technologies and adapting to changes. In conclusion, digital literacy is essential for success in today’s world. If we spare no efforts to improve our digital skills, the future of our personal and professional lives will be both hopeful and rosy.","categories":["other"]},{"title":"gym","path":"/2024/12/12/RL/gym/","content":"基础使用 make 加载环境 env_id 为加载环境的名称id render 有“human”, “rgb_array”, “ansi”三种，代表渲染模式。 reset ：重置环境为初始状态 step ：当前状态采用action动作走一步，转移到下一状态。 close ：关闭环境 123456789101112131415161718import gymnasium as gymimport numpy as npenv_id = &quot;Taxi-v3&quot;env = gym.make(env_id , render_mode=&#x27;human&#x27;)observation, info = env.reset()print(env.observation_space)print(env.action_space)episode_over = Falsewhile not episode_over: action = env.action_space.sample() # agent policy that uses the observation and info observation, reward, terminated, truncated, info = env.step(action) episode_over = terminated or truncatedenv.close() 基本函数方法 1.step()","tags":["rl"],"categories":["rl"]},{"title":"pytorch-cuda使用","path":"/2024/12/12/RL/touch-cuda/","content":"Conda本地安装指令 1conda install --use-local 包名 将下载的包放到conda安装位置的pkgs文件夹中。 数据加载 所有自定义数据集都需要继承Dataset类，并重写__getitem__\\_\\_getitem\\_\\___getitem__方法和lenlenlen方法 123456789101112131415161718192021222324252627282930313233343536373839from torch.utils.data import Datasetimport matplotlib.pyplot as pltimport osimport cv2class MyDataset(Dataset): def __init__(self,root_dir,label_dir): self.root_dir = root_dir self.label_dir = label_dir self.path = os.path.join(self.root_dir,self.label_dir) self.image_path = os.listdir(self.path) def __getitem__(self,idx): image_name = self.image_path[idx] image_path = os.path.join(self.path,image_name) image = cv2.imread(image_path) label = self.label_dir return image,label def __len__(self): return len(self.image_path)if __name__==&#x27;__main__&#x27;: root_dir = &#x27;hymenoptera_data/train/&#x27; ant_dir = &#x27;ants&#x27; bee_dir = &#x27;bees&#x27; AntDataset = MyDataset(root_dir,ant_dir) BeeDataset = MyDataset(root_dir,bee_dir) TrainDataset = AntDataset + BeeDataset img, label = TrainDataset.__getitem__(0) # 将图像从BGR格式转换为RGB格式 rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 使用matplotlib显示图像 plt.imshow(rgb_image) plt.axis(&#x27;off&#x27;) plt.show() GPU/CUDA相关操作 1234567891011121314151617import torch# 检查系统中是否有可用的 GPUif torch.cuda.is_available(): # 获取可用的 GPU 设备数量 num_devices = torch.cuda.device_count() print(&quot;可用 GPU 数量:&quot;, num_devices) # 遍历所有可用的 GPU 设备并打印详细信息 for i in range(num_devices): device = torch.cuda.get_device_properties(i) print(f&quot; GPU &#123;i&#125; 的详细信息:&quot;) print(&quot;名称:&quot;, device.name) print(&quot;计算能力:&quot;, f&quot;&#123;device.major&#125;.&#123;device.minor&#125;&quot;) print(&quot;内存总量 (GB):&quot;, round(device.total_memory / (1024**3), 1))else: print(&quot;没有可用的 GPU&quot;) 123456可用 GPU 数量: 1GPU 0 的详细信息:名称: NVIDIA GeForce RTX 3060 Laptop GPU计算能力: 8.6内存总量 (GB): 6.0 pytorch to函数 在PyTorch中，可以使用to()方法将Tensor或模型移动到指定的设备上。 12self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) 12states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) 1 2 3 尺寸 6 7 8 9"},{"title":"时序查分","path":"/2024/12/11/RL/TD/","content":"Sarsa算法 步骤","tags":["rl"],"categories":["rl"]},{"title":"动态规划算法","path":"/2024/12/11/RL/dp/","content":"简介 基于动态规划的强化学习算法主要有两种：一是策略迭代（policy iteration），二是价值迭代（value iteration）。其中，策略迭代由两部分组成：策略评估（policy evaluation）和策略提升（policy improvement）。具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。 不同于蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。 策略迭代（Policy_Iteration) 策略评估 根据贝尔曼期望方程，用用上一轮的状态价值函数来计算当前这一轮的状态价值函数。 Vk+1(s)=∑a∈Aπ(a∣s)(r(s,a)+γ∑s′∈SP(s′∣s,a)Vk(s′))V^{k+1}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) V^{k}\\left(s^{\\prime}\\right)\\right) Vk+1(s)=a∈A∑​π(a∣s)(r(s,a)+γs′∈S∑​P(s′∣s,a)Vk(s′)) 策略提升 在当前状态采取新动作得到的价值大于当前状态价值，因此可以更新策略，获取更高价值。 Vπ′(s)≥Vπ(s)V^{\\pi^{\\prime}}(s) \\geq V^{\\pi}(s) Vπ′(s)≥Vπ(s)","tags":["rl"],"categories":["rl"]},{"title":"2024数学建模国赛","path":"/2024/09/03/计算机基础/数学建模2024/","content":"第二问思路 是否对配件检验： 如果对配件检验：得到的配件一定是合格品，只需计算平均价值。 E=(w+c)pE = \\frac{(w+c)}{p}E=p(w+c)​ ppp表示合格品概率，www表示配件购买价格，ccc表示配件检验成本 不对配件检验：会提高成品的不合格率 是否对成品检验： 对成品检验：需要付出检验费用，但卖出的成品一定是合格品，所以不需要支付退还费用。 成本构成：1.合格品的成本（包括配件购买费用+配件检验费用） ；2.组装费用； 3.成品检验费用 ；4.不合格品的成本 对成品不检验：不需要检验费用，但卖出的成品有一部分是不合格品，需要支付退还费用。 成本构成： 枚举所有做法（第二问） 首先考虑不拆解的情况 不检验配件A，不检验配件B，不检验成品，不拆解成品。 成品为合格品概率：p(qualified)=pa∗pb∗pcp(qualified) = p_a * p_b * p_cp(qualified)=pa​∗pb​∗pc​ 不合格品概率：1−p(qualified)1 - p(qualified)1−p(qualified) 平均收益：E=p(qualified)∗w−1∗(cost)−(1−p(qualified))∗(swap)E = p(qualified) * w - 1 * (cost) - (1-p(qualified))*(swap)E=p(qualified)∗w−1∗(cost)−(1−p(qualified))∗(swap) 检验配件A，不检验配件B，不检验成品，不拆解成品。 配件A的平均成本：cost+cp\\frac{cost + c}{p}pcost+c​ 成品为合格品概率：p(qualified)=pb∗pcp(qualified) = p_b * p_cp(qualified)=pb​∗pc​ 平均收益：E=p(qualified)∗w−1∗cost−(1−qualified)∗swapE = p(qualified) * w - 1*cost - (1-qualified)*swapE=p(qualified)∗w−1∗cost−(1−qualified)∗swap 组成一件成品的价格：cost=costa+costb+evala+assemblecost = cost_a + cost_b + eval_a + assemblecost=costa​+costb​+evala​+assemble 不检验配件A，检验配件B，不检验成品，不拆解成品。 原理同上 检验配件A，检验配件B，不检验成品，不拆解成品。 合格品概率：p(qualified)=pcp(qualified) = p_cp(qualified)=pc​ 平均收益：E=p(qualified)∗w−1∗cost−(1−p(qualified))∗swapE = p(qualified) * w - 1 * cost - (1-p(qualified))*swapE=p(qualified)∗w−1∗cost−(1−p(qualified))∗swap 生产价格：cost=E(costA)+E(costB)+assemblecost = E(cost_A) + E(cost_B) + assemblecost=E(costA​)+E(costB​)+assemble 综上所述：当不检验成品，且不拆解成品时，卖出每件产品的期望收益为： E=p(qualified)∗w−1∗cost−p(unqualified)∗swapE = p(qualified) * w - 1*cost - p(unqualified) * swapE=p(qualified)∗w−1∗cost−p(unqualified)∗swap 只是每种情况，计算生产成本的结果不同 不检验配件A，不检验配件B，检验成品，不拆解成品。 合格品概率：p(qualified)=pA∗pB∗pCp(qualified) = p_A * p_B * p_Cp(qualified)=pA​∗pB​∗pC​ 销售收益：p(qualified)∗wp(qualified) * wp(qualified)∗w 生产成本：costA+costB+assemble+evalCcost_A + cost_B + assemble + eval_CcostA​+costB​+assemble+evalC​ 检验配件A，不检验配件B，检验成品，不拆解成品。 合格品概率：p(qualified)=pB∗pCp(qualified) = p_B * p_Cp(qualified)=pB​∗pC​ 销售收益：p(qualified)∗wp(qualified) * wp(qualified)∗w 成产成本：E(costA)+costB+assemble+evalCE(cost_A) + cost_B + assemble + eval_CE(costA​)+costB​+assemble+evalC​ 平均收益：E=p(qualified)∗w−1∗costE = p(qualified) * w - 1*costE=p(qualified)∗w−1∗cost 不检验配件A，检验配件B，检验成品，不拆解成品 同6 检验配件A，检验配件B，检验成品，不拆解成品 cost=E(costA)+E(costB)+assemble+evalCcost = E(cost_A) + E(cost_B) + assemble + eval_Ccost=E(costA​)+E(costB​)+assemble+evalC​ 综上所述：不拆解成品的情况下，所有方案 卖出一件的预期收益为： E=p(qualified)∗w−1∗cost−p(unqualified)∗swapE = p(qualified) * w - 1 * cost - p(unqualified)*swapE=p(qualified)∗w−1∗cost−p(unqualified)∗swap 是否检验配件，影响costcostcost的计算； 是否检验成品，决定了售出成品的不合格率是否为0，可以减少退还损失。 其次考虑拆解成品的情况 由于拆解的是检验不合格的成品，所以，当拆解成品时，必然伴随着检验成品。 不检验配件A，不检验配件B，检验成品，且拆解成品，且不进行二次检验。 暂时认为，这是一种很糟糕的策略，只有在次品率极低的情况下适用。 检验配件A，检验配件B，检验成品，且拆解成品，无需进行二次检验。 第i次合成成功的成本：cost(i)=E(costA)+E(costB)+i∗assemble+(i−1)∗meltcost(i) = E(cost_A) + E(cost_B) + i*assemble + (i-1)*meltcost(i)=E(costA​)+E(costB​)+i∗assemble+(i−1)∗melt 生产成本：$cost = \\sum_{i=1}^{\\infty} cost(i) $ 成品的合格率：p(qualified)=pcp(qualified) = p_cp(qualified)=pc​ 卖出一件产品的预期收益同上。 检验配件A，不检验配件B，检验成品，拆解成品，并对B进行二次检验 这里需要用到朴素贝叶斯公式！ 成品合格率：p(qualified)=pB∗pCp(qualified) = p_B * p_Cp(qualified)=pB​∗pC​ 仅因为组装问题导致为次品的概率为：(1−pC)∗pB(1-p_C)*p_B(1−pC​)∗pB​ ， 配件B损坏导致为次品的概率（包括了B既为次品，还组装失败的情况）：1−pB1-p_B1−pB​ 由于所有配件都至多检验一次，一旦通过检验，则一定为合格品。 第i次合成成功的花费： cost(i)=E(costA)+costB+i∗assemble+(i−1)∗melt+(i&gt;1)?:0:1∗evalccost(i) = E(cost_A) + cost_B + i*assemble + (i-1)*melt + (i&gt;1)?:0:1 * eval_ccost(i)=E(costA​)+costB​+i∗assemble+(i−1)∗melt+(i&gt;1)?:0:1∗evalc​ 生产成本： cost=pb∗pc∗cost(1)+(1−pb)∗(E(costA)+costB+assemble+evalc)+pb∗(1−pc)(i−1)∗pc∗(E(costA)+costB+i∗assemble+(i−1)∗melt)cost = p_b*p_c*cost(1) + (1-p_b)*(E(cost_A)+cost_B + assemble + eval_c) + p_b * (1-p_c)^{(i-1)} * p_c *(E_(cost_A) + cost_B + i*assemble + (i-1)*melt)cost=pb​∗pc​∗cost(1)+(1−pb​)∗(E(costA​)+costB​+assemble+evalc​)+pb​∗(1−pc​)(i−1)∗pc​∗(E(​costA​)+costB​+i∗assemble+(i−1)∗melt) 4.不检验A，B，检验成品，拆解成品，并对A，B进行二次检验。 cost=∑i=1ncosticost = \\sum_{i=1}^{n} cost_i cost=i=1∑n​costi​","categories":["其他"]},{"title":"rs232","path":"/2024/08/05/fpga/rs232/","content":"理论学习 通信协议基础 ​ 通用异步收发传输器（Universal Asynchronous Receiver/Transmitter），通常称作UART。UART是一种通用的数据通信协议，也是异步串行通信口（串口）的总称，它在发送数据时将并行数据转换成串行数据来传输，在接收数据时将接收到的串行数据转换成并行数据。它包括了RS 232、RS499、RS423、RS422和RS485等接口标准规范和总线标准规范。 ​ 串口作为常用的三大低速总线（UART、SPI、IIC）之一，在设计众多通信接口和调试时占有重要地位。但UART和SPI、IIC不同的是，它是异步通信接口，异步通信中的接收方并不知道数据什么时候会到达，所以双方收发端都要有各自的时钟，在数据传输过程中是不需要时钟的，发送方发送的时间间隔可以不均匀，接受 方是在数据的起始位和停止位的帮助下实现信息同步的。而SPI、IIC是同步通信接口（后面的章节会做详细介绍），同步通信中双方使用频率一致的时钟，在数据传输过程中时钟伴随着数据一起传输，发送方和接收方使用的时钟都是由主机提供的。 UART基础 ​ UART通信只有两根信号线，一根是发送数据端口线叫tx（Transmitter），一根是接收数据端口线叫rx（Receiver），对于PC来说它的tx要和对于FPGA来说的rx连接，同样PC的rx要和FPGA的tx连接。UART可以实现全双工，即可以同时进行发送数据和接收数据。 RS232优点 ​ 串口RS232传输数据的距离虽然不远，传输速率也相对较慢，但是串口依然被广泛的用于电路系统的设计中，串口的好处主要表现在以下几个方面： 很多传感器芯片或CPU都带有串口功能，目的是在使用一些传感器或CPU时可以通过串口进行调试，十分方便； 在较为复杂的高速数据接口和数据链路集合的系统中往往联合调试比较困难，可以先使用串口将数据链路部分验证后，再把串口换成高速数据接口。如在做以太网相关的项目时，可以在调试时先使用串口把整个数据链路调通，然后再把串口换成以太网的接口； 串口的数据线一共就两根，也没有时钟线，节省了大量的管脚资源。 RS232信号线 ​ 在旧式的台式计算机中一般会有 RS-232 标准的 COM 口(也称 DB9 接口)。 名称 符号 数据方向 说明 载波检测 DCD DTE→DCE Data Carrier Detect,数据载波检测，用于 DTE告知对方，本机是否收到对方的载波信 号 接收数据 RXD DTE&lt;DCE ReceiveData,数据接收信号，即输入。 发送数据 TXD DTE→DCE Transmit Data,数据发送信号，即输出。两个 设备之间的TXD与RXD应交叉相连 数据终端 (DTE)就 绪 DTR DTE→DCE Data Terminal Ready,数据终端就绪，用于 DTE向对方告知本机是否已准备好 信号地 GND 地线，两个通讯设备之间的地电位可能不一 样，这会影响收发双方的电平信号，所以两 个串口设备之间必须要使用地线连接，即共 地。 数据设备 (DCE)就 绪 DSR DTE&lt;DCE Data Set Ready,数据发送就绪，用于DCE告 知对方本机是否处于待命状态 请求发送 RTS DTE→DCE Request To Send,请求发送，DTE请求DCE 本设备向DCE端发送数据 允许发送 CTS DTE&lt;DCE Clear To Send,允许发送，DCE回应对方的 RTS发送请求，告知对方是否可以发送数据 响铃指示 RI DTE&lt;DCE Ring Indicator,响铃指示，表示DCE端与线 路已接通 RS232协议基础 RS232帧结构 ​\t在没有数据传输时，信道持续传输1，知道遇到一位0（起始位），标志着一帧数据的传输。 波特率：在信息传输通道中，携带数据信息的信号单元叫码元（因为串口是1bit进行传输的，所以其码元就是代表一个二进制数），每秒钟通过信号传输的码元数称为码元的传输速率，简称波特率，常用符号“Baud”表示，其单位为“波特每秒（Bps）”。串口常见的波特率有4800、9600、115200等。 比特率：每秒钟通信信道传输的信息量称为位传输速率，简称比特率，其单位为“每秒比特数（bps）”。比特率可由波特率计算得出，公式为：比特率=波特率 * 单个调制状态对应的二进制位数。如果使用的是9600的波特率，其串口的比特率为：9600Bps * 1bit= 9600bps。 由计算得串口发送或者接收1bit数据的时间为一个波特，即1/9600秒，如果用50MHz（周期为20ns）的系统时钟来计数，需要计数的个数为cnt = (1s * 10^9)ns / 9600bit)ns / 20ns ≈ 5208个系统时钟周期，即每个bit数据之间的间隔要在50MHz的时钟频率下计数5208次。 项目实战 RS232 接收端模块 该模块功能为：将串行数据转化为8位并行数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124module uart_rx // rs232接受模块 ： 需要把串行数据转化为并行数据#(\tparameter UART_BPS = &#x27;d9600, //串口波特率\tparameter CLK_FREQ = &#x27;d50_000_000 //时钟频率)(\tinput sys_clk,sys_rst_n, input rx,\toutput reg po_flag,\toutput reg [7:0] po_data);parameter BAUD_CNT_MAX = CLK_FREQ / UART_BPS;reg rx_reg1,rx_reg2,rx_reg3;\t// 将po_data打两拍消除亚稳态，reg3再打一拍检测下降沿（起始位）reg start_neg;\t// 数据开始标志位reg work_en;\t// 工作状态reg [12:0] baud_cnt;\t// 波特计数器reg bit_flag; // bit加信号reg [3:0] bit_cnt; // bit计数器reg rx_flag; // 输出传输完成信号reg [7:0] rx_data; // 数据移位寄存器// 第一级寄存器，复位状态为1 : // 当没有数据传输时，信道一直传输1，直到发现起始位0，产生一个下降沿，表示数据传输开始always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg1 &lt;= 1&#x27;b1;\telse rx_reg1 &lt;= rx;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg2 &lt;= 1&#x27;b1;\telse rx_reg2 &lt;= rx_reg1;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg3 &lt;= 1&#x27;b1;\telse rx_reg3 &lt;= rx_reg2;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) start_neg &lt;= 1&#x27;b0;\telse if(rx_reg2 == 1&#x27;b0 &amp;&amp; rx_reg3 == 1&#x27;b1 &amp;&amp; work_en == 1&#x27;b0) start_neg &lt;= 1&#x27;b1;\telse start_neg &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) work_en &lt;= 1&#x27;b0;\telse if(start_neg == 1&#x27;b1) work_en &lt;= 1&#x27;b1;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) work_en &lt;= 1&#x27;b0;\telse work_en &lt;= work_en;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) baud_cnt &lt;= 13&#x27;b0;\telse if(work_en == 1&#x27;b1) begin if(baud_cnt == BAUD_CNT_MAX - 1) baud_cnt &lt;= 13&#x27;b0; else baud_cnt &lt;= baud_cnt + 1; end\telse baud_cnt &lt;= 13&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_flag &lt;= 1&#x27;b0;\telse if(baud_cnt == (BAUD_CNT_MAX &gt;&gt; 1)) bit_flag &lt;= 1&#x27;b1;\telse bit_flag &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_flag == 1&#x27;b1) begin if(bit_cnt == 4&#x27;d8) bit_cnt &lt;= 4&#x27;b0; else bit_cnt &lt;= bit_cnt + 1; end\telse bit_cnt &lt;= bit_cnt;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_data &lt;= 8&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; (bit_cnt &gt;= 1 &amp;&amp; bit_cnt &lt;= 8)) rx_data &lt;= &#123;rx_reg3,rx_data[7:1]&#125;;\telse rx_data &lt;= rx_data;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_flag &lt;= 1&#x27;b0;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) rx_flag &lt;= 1&#x27;b1;\telse rx_flag &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_data &lt;= 8&#x27;b0;\telse if(rx_flag == 1&#x27;b1) po_data &lt;= rx_data;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_flag &lt;= 1&#x27;b0;\telse if(rx_flag == 1&#x27;b1) po_flag &lt;= rx_flag;endmodule RS232发送端模块 该模块功能为：将8位并行数据转化为串行数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980module uart_tx\t// rs232发送模块，把并行数据转化为串行#(\tparameter UART_BPS = &#x27;d9600,\tparameter CLK_FREQ = &#x27;d50_000_000)(\tinput sys_clk,sys_rst_n, input pi_flag, input [7:0] pi_data,\toutput reg tx );parameter BAUD_CNT_MAX = CLK_FREQ / UART_BPS;reg [12:0] baud_cnt;reg bit_flag;reg [3:0] bit_cnt;reg work_en;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) work_en &lt;= 1&#x27;b0;\telse if(pi_flag == 1&#x27;b1) work_en &lt;= 1&#x27;b1;\telse if(bit_cnt == 4&#x27;d9 &amp;&amp; bit_flag == 1&#x27;b1) work_en &lt;= 1&#x27;b0;\telse work_en &lt;= work_en;always@(posedge sys_clk or negedge sys_rst_n) if(sys_rst_n == 1&#x27;b0) baud_cnt &lt;= 13&#x27;b0; else if(work_en == 1&#x27;b1) begin if(baud_cnt == BAUD_CNT_MAX - 1) baud_cnt &lt;= 13&#x27;b0; else baud_cnt &lt;= baud_cnt + 1; end else baud_cnt &lt;= 13&#x27;b0; always@(posedge sys_clk or negedge sys_rst_n) if(sys_rst_n == 1&#x27;b0) bit_flag &lt;= 1&#x27;b0; else if(baud_cnt == 1) bit_flag &lt;= 1&#x27;b1; else bit_flag &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; bit_cnt == 4&#x27;d9) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; work_en == 1&#x27;b1) bit_cnt &lt;= bit_cnt + 1;\telse bit_cnt &lt;= bit_cnt;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) tx &lt;= 1&#x27;b1;\telse if(bit_flag == 1&#x27;b1) case(bit_cnt) 0 : tx &lt;= 1&#x27;b0; 1 : tx &lt;= pi_data[0]; 2 : tx &lt;= pi_data[1]; 3 : tx &lt;= pi_data[2]; 4 : tx &lt;= pi_data[3]; 5 : tx &lt;= pi_data[4]; 6 : tx &lt;= pi_data[5]; 7 : tx &lt;= pi_data[6]; 8 : tx &lt;= pi_data[7]; 9 : tx &lt;= 1&#x27;b1; default : tx &lt;= 1&#x27;b1; endcaseendmodule 学习链接 笔记摘自：野火fpga开发实战指南：串口RS232 其他： 详解 | 还不懂串口通信？看这篇！","categories":["fpga"]},{"title":"rs485","path":"/2024/08/05/fpga/rs485/","content":"理论基础 RS-485是双向、半双工通信协议，信号采用差分传输方式，允许多个驱动器和接收器挂接在总线上，其中每个驱动器都能够脱离总线。 RS-232是双向、全双工通信协议，信号采用单端传输方式。 差分传输有更好的抗干扰能力。 其次，485相对232可以进行长距离的信号传输；使用收发器，对电压敏感（200mv），传输距离（1200m），传输最大速率（10mb/s）。但485只支持半双工。","categories":["fpga"]},{"title":"基于rs232的lcd图像显示","path":"/2024/08/05/fpga/uart_lcd/","content":"1.使用uart_rx模块作为图像接受模块 波特率的计算 波特率：每秒钟通过信号传输的码元数称为码元的传输速率 使用时钟频率 除以 波特率 可以得到传输每个码源所占用的时钟周期数 $baud_cnt_max = \\frac{时钟频率}{波特率} $ 为什么要做打两拍操作 消除亚稳态，稳定信号。 在找下降沿时，不能使用rx_reg1rx\\_reg1rx_reg1 和rx_reg2rx\\_reg2rx_reg2 求得，因为rx_reg1rx\\_reg1rx_reg1 信号不稳定，使用其求得的结果也不稳定，因此将 rx_reg2rx\\_reg2rx_reg2再打一拍，使用rx_reg2rx\\_reg2rx_reg2 和 rx_reg3rx\\_reg3rx_reg3 求得。 代码中 bit_cntbit\\_cntbit_cnt 只计数到8 这是因为没有计数停止位，所以只有0~8 共计数 9位（1位起始位+8位数据位）。 rs232数据移位方向： 最先传输的比特在低位，最后传输的比特在高位 PC机通过串口调试助手往FPGA发8bit数据时，FPGA通过串口线rx一位一位地接收，从最低位到最高位依次接收，最后在FPGA里面位拼接成8比特数据。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119module uart_rx#parameter(\tparameter UART_BPS = &#x27;d9600, CLK_FREQ = &#x27;d50_000_000)(\tinput sys_clk,sys_rst_n, input rx, output reg [7:0] po_data,\toutput reg po_flag);localparam BAUD_CNT_MAX = CLK_FREQ / UART_BPS;reg rx_reg1;reg rx_reg2;reg rx_reg3;reg start_nedge;reg work_en;reg [12:0] baud_cnt;reg bit_flag;reg [3:0] bit_cnt;reg [7:0] rx_data;reg rx_flag;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg1 &lt;= 1&#x27;b0;\telse rx_reg1 &lt;= rx;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg2 &lt;= 1&#x27;b0;\telse rx_reg2 &lt;= rx_reg1;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg3 &lt;= 1&#x27;b0;\telse rx_reg3 &lt;= rx_reg2;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) start_nedge &lt;= 1&#x27;b0;\telse if(rx_reg2 == 1&#x27;b0 &amp;&amp; rx_reg3 == 1&#x27;b1) start_nedge &lt;= 1&#x27;b1;\telse start_nedge &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) work_en &lt;= 1&#x27;b0;\telse if(start_nedge == 1&#x27;b1)\t// 开始工作条件 work_en &lt;= 1&#x27;b1;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) // 停止工作条件：完成一个字节的传输 work_en &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) baud_cnt &lt;= 13&#x27;b0;\telse if(baud_cnt == BAUD_CNT_MAX - 1 &amp;&amp; work_en == 1&#x27;b1) baud_cnt &lt;= 13&#x27;b0;\telse if(work_en == 1&#x27;b1) baud_cnt &lt;= baud_cnt + 1;\telse baud_cnt &lt;= 13&#x27;b0;\t// 传输完成，停止工作，波特计数器清零always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_flag &lt;= 1&#x27;b0;\telse if(baud_cnt == BAUD_CNT_MAX / 2 - 1) bit_flag &lt;= 1&#x27;b1;\telse bit_flag &lt;= 1&#x27;b0;// 这里由于没有计数停止位，所以只对0~8计数// 传输停止位时已经将work_en拉低，不会使bit_flag有效，因此bit_cnt也不会计数always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) bit_cnt &lt;= 4&#x27;d0;\telse if(bit_flag == 1&#x27;b1) bit_cnt &lt;= bit_cnt + 1;// 对数据进行移位always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_data &lt;= 8&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; bit_cnt &gt;= 4&#x27;d1 &amp;&amp; bit_cnt &lt;= 4&#x27;d8) rx_data = &#123;rx_reg3,rx_data[7:1]&#125;;// 传输完8比特，拉高rx_flagalways@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_flag &lt;= 1&#x27;b0;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1)\t// 8位的最后一个比特传输完成 rx_flag &lt;= 1&#x27;b1;\telse rx_flag &lt;= 1&#x27;b0;// po_data 和 po_flag 都以 rx_data 为基准，可以实现统一时间输出always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_data &lt;= 8&#x27;b0;\telse if(rx_flag == 1&#x27;b1) po_data &lt;= rx_data;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_flag &lt;= 1&#x27;b0;\telse po_flag &lt;= rx_flag;endmodule","categories":["fpga"]},{"title":"Sublime快捷键","path":"/2024/08/03/工具使用基础/sublime快捷键/","content":"Sublime快捷键 查找替换 查找 ： Ctrl + F 查找替换 ：Ctrl + H","categories":["计算机基础"]},{"title":"Git基础","path":"/2024/08/03/计算机基础/Git/","content":"一.git基础 初始化git仓库 1git init 添加文件 1git add readme.txt 提交文件 1git commit -m &quot;submit infomation&quot; 查看工作区状态 1git status 查看修改内容 1git diff 查看提交历史 12git loggit reflog 回退版本 1git reset --hard 7376(版本号) 撤销暂存区的修改 1git reset HEAD readme.txt 撤销工作区的修改 1git checkout -- readme.txt 删除版本库的文件 1git rm readme.txt 管理github仓库 123git remote add origin git@github.com:skyang1/learngit.gitgit remote -v // 查看关联仓库 推送本地库的所有内容到github 1git push -u origin master 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 克隆github项目 1git clone git@github.com:LeiWang1999/FPGA.git 二、分支管理 查看分支 1git branch 创建分支 1git branch dev 切换分支 12git switch devgit checkout dev 创建并切换分支 12git switch -c devgit checkout -b dev 合并某分支到当前分支 1git merge dev 删除分支 1git branch -d dev","tags":["git","linux"],"categories":["计算机基础"]},{"path":"/about/index.html","content":"下面写关于自己的内容"},{"title":"我的朋友们","path":"/friends/index.html","content":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"Hexo-stellar配置","path":"/wiki/diary/example.html","content":"今天主要问题在文档的配置，按照官方教程无法配置成功，更改项目结构后配置成功。 新的项目结构为： wiki.yml ：文档目录，存放在source/_data目录下- diary 1- diary diary.yml ：日记文档的配置文件，存放到source/_data/wiki目录下 123456789101112131415161718192021222324252627name: Diarytitle: skyang-学习日记subtitle: &#x27;每个人的独立博客 | Designed by xaoxuu&#x27;tags: 日记icon: /assets/wiki/stellar/icon.svgcover: /assets/wiki/stellar/icon.svgdescription: skyang学习日记-每天学习的一些零碎的知识，难以整理，写为日记# repo: xaoxuu/hexo-theme-stellarsearch: filter: /wiki/stellar/ placeholder: 在 Stellar 中搜索...leftbar: - tree - timeline_stellar_releases - related# comment_title: &#x27;评论区仅供交流，有问题请提 [issue](https://github.com/xaoxuu/hexo-theme-stellar/issues) 反馈。&#x27;# comments:# service: giscus# giscus:# data-repo: xaoxuu/hexo-theme-stellar# data-mapping: number# data-term: 226base_dir: /wiki/stellar/tree: &#x27;快速开始&#x27;: - index\t- day_20241210 day_20241210.md ：文档具体内容，存放在myblog/wiki/diary路径下"},{"title":"npm安装软件包","path":"/wiki/diary/npm.html","content":"npm安装软件包 全局安装 1nmp install hexo -g 安装的软件包在NodeJS安装目录下的global_modules文件夹下 本地安装 安装到当前路径的文件夹下 由于没有配置全局环境变量，可以使用 npx 指令使用安装的软件包 1npx hexo g -d"},{"title":"日记首页","path":"/wiki/diary/index.html","content":"记录一下每日学习日常，以零散笔记为主。"},{"title":"Hexo-stellar配置","path":"/wiki/diary/stellar.html","content":"今天主要问题在文档的配置，按照官方教程无法配置成功，更改项目结构后配置成功。 新的项目结构为： wiki.yml ：文档目录，存放在source/_data目录下- diary 1- diary diary.yml ：日记文档的配置文件，存放到source/_data/wiki目录下 123456789101112131415161718192021222324252627name: Diarytitle: skyang-学习日记subtitle: &#x27;每个人的独立博客 | Designed by xaoxuu&#x27;tags: 日记icon: /assets/wiki/stellar/icon.svgcover: /assets/wiki/stellar/icon.svgdescription: skyang学习日记-每天学习的一些零碎的知识，难以整理，写为日记# repo: xaoxuu/hexo-theme-stellarsearch: filter: /wiki/stellar/ placeholder: 在 Stellar 中搜索...leftbar: - tree - timeline_stellar_releases - related# comment_title: &#x27;评论区仅供交流，有问题请提 [issue](https://github.com/xaoxuu/hexo-theme-stellar/issues) 反馈。&#x27;# comments:# service: giscus# giscus:# data-repo: xaoxuu/hexo-theme-stellar# data-mapping: number# data-term: 226base_dir: /wiki/stellar/tree: &#x27;快速开始&#x27;: - index\t- day_20241210 day_20241210.md ：文档具体内容，存放在myblog/wiki/diary路径下"},{"title":"java多线程编程","path":"/wiki/java/juc.html","content":"多线程基础 并发：在同一时刻，有多个指令在单个cpu上交替执行。 并行：在同一时刻，有多个指令在多个cpu上同时执行。 多线程实现方式 继承Thread类方式实现 定义一个类继承Thread 重写子类run()方法 创建子类对象并调用进程 12345678public class Mythread extends Thread&#123; @Override public void run() &#123; for(int i=0;i&lt;100;i++)&#123; System.out.println(this.getName() + i); &#125; &#125;&#125; 123456789101112public class Main &#123; public static void main(String[] args)&#123; Mythread th1 = new Mythread(); Mythread th2 = new Mythread(); th1.setName(&quot;thread1:&quot;); th2.setName(&quot;thread2:&quot;); th1.start(); th2.start(); &#125;&#125; 实现Runnable接口实现 自己定义一个类实现Runnable接口 重写里面的run方法 创建自己的类的对象 创建个Thread类的对象，并开启线程 12345678public class Myrun implements Runnable&#123; @Override public void run() &#123; Thread th = Thread.currentThread(); for(int i=0;i&lt;100;i++) System.out.println(th.getName() + i); &#125;&#125; 1234567891011121314151617public class Main &#123; public static void main(String[] args)&#123; // 创建任务对象，表示多线程要执行的任务 Myrun run= new Myrun(); // 创建线程对象 Thread th1 = new Thread(run); Thread th2 = new Thread(run); th1.setName(&quot;Thread1:&quot;); th2.setName(&quot;Thread2:&quot;); th1.start(); th2.start(); &#125;&#125; 使用Callable接口和Furture接口进行实现 创建一个类MyCallab1e实现callab1e按口 重写call(是有返回值的，表示多线程运行的结果) 创建MyCallable的对象（表示多线程要执行的任务） 创建FutureTask的对象（作用管理多线程运行的结果） 创建Thread类的对象，并启动（表示线程） 1234567891011public class Mycallable implements Callable &#123; @Override public Object call() throws Exception &#123; int sum = 0; for(int i=0;i&lt;100;i++)&#123; System.out.println(i); sum = sum + i; &#125; return sum; &#125;&#125; 12345678910111213141516public class Main &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 创建MyCallable对象，表示多线程要执行的任务 Mycallable mc = new Mycallable(); // 创建FureturTask对象，管理多线程运行的结果 FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread th = new Thread(ft); th.start(); // 获取多线程运行的结果 Integer result = ft.get(); System.out.println(result); &#125;&#125; Thread常用成员方法 优先级：1~10，默认为5，仅代表运行概率。 守护线程：当其他的非守护线程执行完毕之后：守护线程会陆续结束 线程生命周期 案例一 模拟使用三个线程表示三个售票窗口，一共出售1000张门票。 Thread实现 注意点： synchronized 应当锁一个相同的对象，可以锁一个静态成员变量 / 本类的class字节码文件。 synchronized应当放到循环之内，否则一个线程循环1000次后才会解锁。 123456789101112131415161718192021package org.example.juc;public class Mythread extends Thread&#123; static int ticket = 0; @Override public void run() &#123; while(true) &#123; synchronized (Mythread.class)&#123; if(ticket == 1000) break; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; ticket++; System.out.println(Thread.currentThread().getName() + &quot;出售了门票&quot; + ticket); &#125; &#125; &#125;&#125; Runnable实现 期间的问题：由于线程切换速度较慢，所以出售100张票可能看不到线程切换的表现，因此将循环次数改为1000。 1234567891011121314151617181920212223public class Myrun implements Runnable&#123; static int ticket = 0; @Override public void run() &#123; while(true) if(method()) break; &#125; private synchronized boolean method()&#123; if(ticket == 1000) return true; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; ticket++; System.out.println(Thread.currentThread().getName() + &quot;出售了门票&quot; + ticket); return false; &#125;&#125; 案例二 普通实现 生产者消费者问题：一位顾客吃面，一位厨师做面，桌子上可以摆一碗面。","tags":[null],"categories":[null]}]