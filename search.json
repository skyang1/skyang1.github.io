[{"title":"基础知识","path":"/2024/12/16/RL/baserl/","content":"有偏估计和无偏估计 有偏估计（biased estimate）是指由样本值求得的估计值与待估参数的真值之间有系统误差，其期望值不是待估参数的真值。 机器学习概率统计知识(1): 无偏估计与有偏估计 简述有偏估计和无偏估计","tags":["rl"],"categories":["rl"]},{"title":"mathmatical-rl","path":"/2024/12/15/RL/mathmatical-rl/","content":"第三章、贝尔曼最优方程 ji=dangongshiji = dangongshiji=dangongshi Definition 3.1 (Optimal policy and optimal state value). A policy $ \\pi^{ * } $ is optimal if vπ∗(s)≥vπ(s)v_{\\pi^{*} } ( s ) \\geq v_{\\pi} ( s )vπ∗​(s)≥vπ​(s) for all s∈Ss \\in\\mathcal{S}s∈S and for any other policy π\\piπ . The state values of π∗\\pi^{*}π∗ are the optimal state values. 第四章、值迭代和策略迭代 Value Iteration Policy update πk+1(s)=arg⁡max⁡π∑aπ(a∣s)(∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)vk(s′))⏟qk(s,a),s∈S.\\pi_{k+1} ( s )=\\operatorname{a r g} \\operatorname* {m a x}_{\\pi} \\sum_{a} \\pi( a | s ) \\underbrace{\\left( \\sum_{r} p ( r | s, a ) r+\\gamma\\sum_{s^{\\prime} } p ( s^{\\prime} | s, a ) v_{k} ( s^{\\prime} ) \\right)}_{q_{k} ( s, a )}, \\quad s \\in{\\mathcal{S} }. πk+1​(s)=argπmax​a∑​π(a∣s)qk​(s,a)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vk​(s′))​​,s∈S. 采用贪心的策略，该状态下，策略选择的动作唯一。倘若最优策略不唯一，任选其中的一个策略不影响算法的收敛性。 πk+1(a∣s)={1,a=ak∗(s),0,a≠ak∗(s),\\pi_{k+1} ( a | s )=\\left\\{\\begin{array} {c c} { {1,} } &amp; { {a=a_{k}^{*} ( s ),} } \\\\ { {0,} } &amp; { {a eq a_{k}^{*} ( s ),} } \\\\ \\end{array} \\right. πk+1​(a∣s)={1,0,​a=ak∗​(s),a=ak∗​(s),​ Value update vk+1(s)=∑aπk+1(a∣s)(∑rp(r∣s,a)r+γ∑s′p(s′∣s,a)vk(s′))⏟qk(s,a),s∈S.v_{k+1} ( s )=\\sum_{a} \\pi_{k+1} ( a | s ) \\underbrace{\\left( \\sum_{r} p ( r | s, a ) r+\\gamma\\sum_{s^{\\prime} } p ( s^{\\prime} | s, a ) v_{k} ( s^{\\prime} ) \\right)}_{q_{k} ( s, a )}, \\quad s \\in\\mathcal{S}. vk+1​(s)=a∑​πk+1​(a∣s)qk​(s,a)(r∑​p(r∣s,a)r+γs′∑​p(s′∣s,a)vk​(s′))​​,s∈S. vk+1(s)=max⁡aqk(s,a).v_{k+1} ( s )=\\operatorname* {m a x}_{a} q_{k} ( s, a ). vk+1​(s)=amax​qk​(s,a). 迭代过程： vk(s)→qk(s,a)→new greedy policy πk+1(s)→new value vk+1(s)=max⁡aqk(s,a)v_k(s) → q_k(s,a) → new \\; greedy \\; policy \\; \\pi_{k+1} ( s ) \\to new \\; value \\; v_{k+1} ( s ) = \\operatorname* {m a x}_{a} q_{k} ( s, a ) vk​(s)→qk​(s,a)→newgreedypolicyπk+1​(s)→newvaluevk+1​(s)=amax​qk​(s,a) 代码 1234567891011121314def value_iteration(env, gamma=1.0): v = np.zeros(env.env.observation_space.n) # initialize value-function max_iterations = 100000 eps = 1e-20 for i in range(max_iterations): prev_v = np.copy(v) for s in range(env.env.observation_space.n): # 在s状态下，采取a动作，转移到s_，概率为p，回报为r。 q_sa = [sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.env.action_space.n)] v[s] = max(q_sa) if (np.sum(np.fabs(prev_v - v)) &lt;= eps): print(&#x27;Value-iteration converged at iteration# %d.&#x27; % (i + 1)) break return v Policy Iteration","tags":["rl"],"categories":["rl"]},{"title":"python","path":"/2024/12/15/RL/python/","content":"torch中函数 torch.view() 定义：调整Tensor的形状 12345678import torchtensor_0 = torch.arange(1, 13)tensor_1 = tensor_0.view(3, 4)tensor_2 = tensor_0.view(-1,6)print(tensor_1)print(tensor_2) 12345tensor([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])tensor([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12]]) torch.gather() 定义：从原tensor中获取指定dim和指定index的数据 用途：方便从批量tensor中获取指定索引下的数据，该索引是高度自定义化的，可乱序的 1torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor 123out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 参考链接：图解PyTorch中的torch.gather函数","tags":["rl"],"categories":["rl"]},{"title":"强化学习算法","path":"/2024/12/15/RL/handsRL/","content":"DQN 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class DQN: &#x27;&#x27;&#x27; DQN算法,包括Double DQN和Dueling DQN &#x27;&#x27;&#x27; def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device, dqn_type=&#x27;VanillaDQN&#x27;): self.action_dim = action_dim if dqn_type == &#x27;DuelingDQN&#x27;: # Dueling DQN采取不一样的网络框架 self.q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device) self.target_q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device) else: self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate) self.gamma = gamma self.epsilon = epsilon self.target_update = target_update self.count = 0 self.dqn_type = dqn_type self.device = device def take_action(self, state): if np.random.random() &lt; self.epsilon: action = np.random.randint(self.action_dim) else: state = torch.tensor([state], dtype=torch.float).to(self.device) action = self.q_net(state).argmax().item() return action def max_q_value(self, state): state = torch.tensor([state], dtype=torch.float).to(self.device) return self.q_net(state).max().item() def update(self, transition_dict): states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[&#x27;actions&#x27;]).view(-1, 1).to( self.device) rewards = torch.tensor(transition_dict[&#x27;rewards&#x27;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[&#x27;next_states&#x27;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[&#x27;dones&#x27;], dtype=torch.float).view(-1, 1).to(self.device) q_values = self.q_net(states).gather(1, actions) if self.dqn_type == &#x27;DoubleDQN&#x27;: max_action = self.q_net(next_states).max(1)[1].view(-1, 1) max_next_q_values = self.target_q_net(next_states).gather( 1, max_action) else: max_next_q_values = self.target_q_net(next_states).max(1)[0].view( -1, 1) q_targets = rewards + self.gamma * max_next_q_values * (1 - dones) dqn_loss = torch.mean(F.mse_loss(q_values, q_targets)) self.optimizer.zero_grad() dqn_loss.backward() self.optimizer.step() if self.count % self.target_update == 0: self.target_q_net.load_state_dict(self.q_net.state_dict()) self.count += 1 DQN与Double DQN区别 高估问题：DQN 存在一个问题，即它倾向于高估 Q 值。这是因为在计算目标 Q 值时，是通过选择下一个状态中 Q 值最大的动作来计算的。由于 Q 值是估计值，存在误差，这种选择最大值的方式会导致高估。 Double DQN 的改进：Double DQN 的主要改进在于它将动作选择和动作评估进行了分离，从而减少 Q 值的高估。在 Double DQN 中，选择下一个动作时使用当前的 Q 网络，而评估这个动作的 Q 值时使用目标 Q 网络。 123456# 下个状态的最大Q值if self.dqn_type == &#x27;DoubleDQN&#x27;: # DQN与Double DQN的区别 max_action = self.q_net(next_states).max(1)[1].view(-1, 1) max_next_q_values = self.target_q_net(next_states).gather(1, max_action)else: # DQN的情况 max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1) Dueling DQN 它的核心思想是将 Q - 网络的输出分为两个部分：状态价值函数VVV和优势函数AAA。QQQ 值函数可以表示为：Q(s,a)=V(s)+A(s,a)Q(s,a) = V(s) + A(s,a)Q(s,a)=V(s)+A(s,a) 状态价值函数V(s)V(s)V(s)表示在状态下的价值，而优势函数A(s,a)A(s,a)A(s,a)表示采取动作相对于平均动作价值在状态下的优势。这种分解的好处是可以更灵活地学习状态价值和动作优势，尤其在某些状态下，动作的影响可能比状态本身的价值变化小，Dueling DQN 能够更好地捕捉这种特性。 12345678910111213class VAnet(torch.nn.Module): &#x27;&#x27;&#x27; 只有一层隐藏层的A网络和V网络 &#x27;&#x27;&#x27; def __init__(self, state_dim, hidden_dim, action_dim): super(VAnet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) # 共享网络部分 self.fc_A = torch.nn.Linear(hidden_dim, action_dim) self.fc_V = torch.nn.Linear(hidden_dim, 1) def forward(self, x): A = self.fc_A(F.relu(self.fc1(x))) V = self.fc_V(F.relu(self.fc1(x))) Q = V + A - A.mean(1).view(-1, 1) # Q值由V值和A值计算得到 return Q 策略梯度算法 假设目标策略πθ\\pi_\\thetaπθ​是一个随机性策略，并且处处可微，其中θ\\thetaθ是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。 J(θ)=Es0[Vπθ(s0)]J(\\theta)=\\mathbb{E}_{s_{0}}\\left[V^{\\pi_{\\theta}}\\left(s_{0}\\right)\\right] J(θ)=Es0​​[Vπθ​(s0​)] 其中，s0s_0s0​表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。 ∇θJ(θ)∝∑s∈Sνπθ(s)∑a∈AQπθ(s,a)∇θπθ(a∣s)=∑s∈Sνπθ(s)∑a∈Aπθ(a∣s)Qπθ(s,a)∇θπθ(a∣s)πθ(a∣s)=Eπθ[Qπθ(s,a)∇θlog⁡πθ(a∣s)]\\begin{aligned} abla_{\\theta} J(\\theta) &amp; \\propto \\sum_{s \\in S} u^{\\pi_{\\theta}}(s) \\sum_{a \\in A} Q^{\\pi_{\\theta}}(s, a) abla_{\\theta} \\pi_{\\theta}(a \\mid s) \\\\ &amp; =\\sum_{s \\in S} u^{\\pi_{\\theta}}(s) \\sum_{a \\in A} \\pi_{\\theta}(a \\mid s) Q^{\\pi_{\\theta}}(s, a) \\frac{ abla_{\\theta} \\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta}(a \\mid s)} \\\\ &amp; =\\mathbb{E}_{\\pi_{\\theta}}\\left[Q^{\\pi_{\\theta}}(s, a) abla_{\\theta} \\log \\pi_{\\theta}(a \\mid s)\\right] \\end{aligned} ∇θ​J(θ)​∝s∈S∑​νπθ​(s)a∈A∑​Qπθ​(s,a)∇θ​πθ​(a∣s)=s∈S∑​νπθ​(s)a∈A∑​πθ​(a∣s)Qπθ​(s,a)πθ​(a∣s)∇θ​πθ​(a∣s)​=Eπθ​​[Qπθ​(s,a)∇θ​logπθ​(a∣s)]​ Reinforce算法流程 123456789class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) 12345678910111213141516171819202122232425262728293031323334class REINFORCE: def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device): self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate) # 使用Adam优化器 self.gamma = gamma # 折扣因子 self.device = device def take_action(self, state): # 根据动作概率分布随机采样 state = torch.tensor([state], dtype=torch.float).to(self.device) probs = self.policy_net(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict): reward_list = transition_dict[&#x27;rewards&#x27;] state_list = transition_dict[&#x27;states&#x27;] action_list = transition_dict[&#x27;actions&#x27;] G = 0 self.optimizer.zero_grad() for i in reversed(range(len(reward_list))): # 从最后一步算起 reward = reward_list[i] state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device) action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device) log_prob = torch.log(self.policy_net(state).gather(1, action)) G = self.gamma * G + reward loss = -log_prob * G # 每一步的损失函数 loss.backward() # 反向传播计算梯度 self.optimizer.step() # 梯度下降 Actor-Critic 1.概述 Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。 Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。 2.策略函数更新 拟合一个值函数来指导策略进行学习 g=E[∑t=0Tψt∇θlog⁡πθ(at∣st)]g=\\mathbb{E} \\left[ \\sum_{t=0}^{T} \\psi_{t} abla_{\\theta} \\operatorname{l o g} \\pi_{\\theta} ( a_{t} | s_{t} ) \\right] g=E[t=0∑T​ψt​∇θ​logπθ​(at​∣st​)] ψ\\psiψ可以有多种形式： ∑t′=0Tγt′rt′\\sum_{t^{\\prime}=0}^{T} \\gamma^{t^{\\prime}} r_{t^{\\prime}}∑t′=0T​γt′rt′​ :轨迹的总回报; ∑t′=tTγt′−trt′\\sum_{t^{\\prime}=t}^{T} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}∑t′=tT​γt′−trt′​ ：动作ata_tat​之后的回报； ∑t′=tTγt′−trt′−b(st)\\sum_{t^{\\prime}=t}^{T} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}-b ( s_{t} )∑t′=tT​γt′−trt′​−b(st​) ：基准线版本的改进； Qπθ(st,at)Q^{\\pi_{\\theta}} ( s_{t}, a_{t} )Qπθ​(st​,at​) : 动作价值函数; Aπθ(st,at)A^{\\pi_{\\theta}} ( s_{t}, a_{t} )Aπθ​(st​,at​) ：优势函数； rt+γVπθ(st+1)−Vπθ(st)r_{t}+\\gamma V^{\\pi_{\\theta}} ( s_{t+1} )-V^{\\pi_{\\theta}} ( s_{t} )rt​+γVπθ​(st+1​)−Vπθ​(st​) :时序差分残差。 3. 价值函数更新 我们将 Critic 价值网络表示为VwV_wVw​，参数为$w $。于是，我们可以采取时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数： L(ω)=12(r+γVω(st+1)−Vω(st))2\\mathcal{L} ( \\omega)=\\frac{1} {2} ( r+\\gamma V_{\\omega} ( s_{t+1} )-V_{\\omega} ( s_{t} ) )^{2} L(ω)=21​(r+γVω​(st+1​)−Vω​(st​))2 将上式中r+γVω(st+1)r+\\gamma V_{\\omega} ( s_{t+1} )r+γVω​(st+1​)作为时序差分目标，不会产生梯度来更新价值函数。因此，价值函数的梯度为： ∇ωL(ω)=−(r+γVω(st+1)−Vω(st))∇ωVω(st) abla_{\\omega} \\mathcal{L} ( \\omega)=-( r+\\gamma V_{\\omega} ( s_{t+1} )-V_{\\omega} ( s_{t} ) ) abla_{\\omega} V_{\\omega} ( s_{t} ) ∇ω​L(ω)=−(r+γVω​(st+1​)−Vω​(st​))∇ω​Vω​(st​) 4.算法流程 5.代码 123456789class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) 123456789class ValueNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim): super(ValueNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, 1) def forward(self, x): x = F.relu(self.fc1(x)) return self.fc2(x) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class ActorCritic: def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device): # 策略网络 self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.critic = ValueNet(state_dim, hidden_dim).to(device) # 价值网络 # 策略网络优化器 self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr) self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) # 价值网络优化器 self.gamma = gamma self.device = device def take_action(self, state): state = torch.tensor([state], dtype=torch.float).to(self.device) probs = self.actor(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict): states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[&#x27;actions&#x27;]).view(-1, 1) .to(self.device) rewards = torch.tensor(transition_dict[&#x27;rewards&#x27;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[&#x27;next_states&#x27;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[&#x27;dones&#x27;], dtype=torch.float).view(-1, 1).to(self.device) # 时序差分目标 td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones) td_delta = td_target - self.critic(states) # 时序差分误差 log_probs = torch.log(self.actor(states).gather(1, actions)) actor_loss = torch.mean(-log_probs * td_delta.detach()) # 均方误差损失函数 critic_loss = torch.mean( F.mse_loss(self.critic(states), td_target.detach())) self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() actor_loss.backward() # 计算策略网络的梯度 critic_loss.backward() # 计算价值网络的梯度 self.actor_optimizer.step() # 更新策略网络的参数 self.critic_optimizer.step() # 更新价值网络的参数 Trpo算法 1.概述 TRPO（Trust Region Policy Optimization）算法一种基于策略梯度（Policy Gradient）的强化学习算法。它主要是为了解决策略梯度方法在优化过程中由于步长选择不当而导致的性能下降问题。 TRPO 基于信赖域（Trust Region）的思想，试图在一个 “可信赖” 的区域内对策略进行优化。这个信赖域是通过限制新策略和旧策略之间的差异来定义的，以确保策略的更新不会过于激进，从而保证策略性能的稳定提升。它的核心是在优化目标函数（通常是累计奖励的期望）的同时，满足一个约束条件，这个约束条件与新旧策略之间的 **KL - 散度（Kullback - Leibler Divergence）**有关。KL - 散度用于衡量两个概率分布之间的差异，在这里用于衡量新旧策略之间的距离。 2.基础知识 2.1KL散度 KL散度（Kullback-Leibler divergence），可以以称作相对熵（relative entropy）或信息散度（information divergence）。KL散度的理论意义在于度量两个概率分布之间的差异程度，当KL散度越大的时候，说明两者的差异程度越大；而当KL散度小的时候，则说明两者的差异程度小。如果两者相同的话，则该KL散度应该为0。 离散随机变量，P对Q的散度： DKL(P∣∣Q)=∑iP(i)ln⁡(P(i)Q(i))\\mathbb{D}_{\\mathrm{K L}} ( P | | Q )=\\sum_{i} P ( i ) \\operatorname{l n} ( \\frac{P ( i )} {Q ( i )} ) DKL​(P∣∣Q)=i∑​P(i)ln(Q(i)P(i)​) 连续随机变量，P对Q的散度： DKL(P∣∣Q)=∫−∞∞p(x)ln⁡(p(x)q(x))dx\\mathbb{D}_{\\mathrm{K L}} ( P | | Q )=\\int_{-\\infty}^{\\infty} p ( {\\bf x} ) \\operatorname{l n} ( \\frac{p ( {\\bf x} )} {q ( {\\bf x} )} ) d {\\bf x} DKL​(P∣∣Q)=∫−∞∞​p(x)ln(q(x)p(x)​)dx 参考链接： [1]机器学习_KL散度详解 [2]关于KL散度（Kullback-Leibler Divergence）的笔记 2.2共轭梯度法 参考资料： [1]共轭梯度法简介","tags":["rl"],"categories":["rl"]},{"title":"cet-6","path":"/2024/12/14/其他/cet-6/","content":"模板 第一段： 1There is a growing awareness of the importance of __. In the contemporary world , __ have/has been increasingly important. It&#x27;s great neccesity for __ to __. Reasons and concrete evidence to support my viewpoint are as follows. 第二段 12345678In the first place,there is no doubt that __.Based on big data , most of __ admitted that __.Moreover, no one can deny that __.Where there is/are __ , there is/ara __.Last but not least,I firmly believe that __.The more fans you have , the happier you are. 第三段 1In conclusion, __. If we spare no efforts to __ , the furture of __ will be both hopeful and rosy. 范文 12345There is a growing awareness of the importance of digital literacy and skills in today&#x27;s world. In the contemporary world, digital literacy has become increasingly important.It&#x27;s of great necessity for individuals to equip themselves with digital skills to thrive in both personal and professional settings. Reasons and concrete evidence to support my viewpoint are as follows.In the first place,there is no doubt that digital literacy enhances productivity and efficiency.Based on big data,most professionals admitted that they have spent 2/3 of their time utilizing digital tools and platforms to perform their tasks.Moreover,no one can deny that digital literacy opens up numerous opportunities for career advancement.Where there are strong digital skills,there are competitive advantages in the job market.Last but not least,I firmly believe that digital literacy fosters lifelong learning and adaptability.The more digitally literate you are,the more capable you are of embracing new technologies and adapting to changes.In conclusion,digital literacy is essential for success in today&#x27;s world.If we spare no efforts to improve our digital skills,the future of our personal and professional lives will be both hopeful and rosy. There is a growing awareness of the importance of digital literacy and skills in today’s world. It’s of great necessity for individuals to equip themselves with digital skills to thrive in both personal and professional settings. Reasons and concrete evidence to support my viewpoint are as follows. In the first place, there is no doubt that digital literacy enhances productivity and efficiency. Based on big data, most professionals admitted that they have spent 2/3 of their time utilizing digital tools and platforms to perform their tasks. Moreover, no one can deny that digital literacy opens up numerous opportunities for career advancement. Where there are strong digital skills,there are competitive advantages in the job market. Last but not least, I firmly believe that digital literacy fosters lifelong learning and adaptability. The more digitally literate you are, the more capable you are of embracing new technologies and adapting to changes. In conclusion, digital literacy is essential for success in today’s world. If we spare no efforts to improve our digital skills, the future of our personal and professional lives will be both hopeful and rosy.","categories":["other"]},{"title":"gym","path":"/2024/12/12/RL/gym/","content":"基础使用 make 加载环境 env_id 为加载环境的名称id render 有“human”, “rgb_array”, “ansi”三种，代表渲染模式。 reset ：重置环境为初始状态 step ：当前状态采用action动作走一步，转移到下一状态。 close ：关闭环境 123456789101112131415161718import gymnasium as gymimport numpy as npenv_id = &quot;Taxi-v3&quot;env = gym.make(env_id , render_mode=&#x27;human&#x27;)observation, info = env.reset()print(env.observation_space)print(env.action_space)episode_over = Falsewhile not episode_over: action = env.action_space.sample() # agent policy that uses the observation and info observation, reward, terminated, truncated, info = env.step(action) episode_over = terminated or truncatedenv.close() 基本函数方法 1.step()","tags":["rl"],"categories":["rl"]},{"title":"pytorch-cuda使用","path":"/2024/12/12/RL/touch-cuda/","content":"GPU/CUDA相关操作 1234567891011121314151617import torch# 检查系统中是否有可用的 GPUif torch.cuda.is_available(): # 获取可用的 GPU 设备数量 num_devices = torch.cuda.device_count() print(&quot;可用 GPU 数量:&quot;, num_devices) # 遍历所有可用的 GPU 设备并打印详细信息 for i in range(num_devices): device = torch.cuda.get_device_properties(i) print(f&quot; GPU &#123;i&#125; 的详细信息:&quot;) print(&quot;名称:&quot;, device.name) print(&quot;计算能力:&quot;, f&quot;&#123;device.major&#125;.&#123;device.minor&#125;&quot;) print(&quot;内存总量 (GB):&quot;, round(device.total_memory / (1024**3), 1))else: print(&quot;没有可用的 GPU&quot;) 123456可用 GPU 数量: 1GPU 0 的详细信息:名称: NVIDIA GeForce RTX 3060 Laptop GPU计算能力: 8.6内存总量 (GB): 6.0 pytorch to函数 在PyTorch中，可以使用to()方法将Tensor或模型移动到指定的设备上。 12self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) 12states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device)"},{"title":"时序查分","path":"/2024/12/11/RL/TD/","content":"Sarsa算法 步骤","tags":["rl"],"categories":["rl"]},{"title":"动态规划算法","path":"/2024/12/11/RL/dp/","content":"简介 基于动态规划的强化学习算法主要有两种：一是策略迭代（policy iteration），二是价值迭代（value iteration）。其中，策略迭代由两部分组成：策略评估（policy evaluation）和策略提升（policy improvement）。具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。 不同于蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。 策略迭代（Policy_Iteration) 策略评估 根据贝尔曼期望方程，用用上一轮的状态价值函数来计算当前这一轮的状态价值函数。 Vk+1(s)=∑a∈Aπ(a∣s)(r(s,a)+γ∑s′∈SP(s′∣s,a)Vk(s′))V^{k+1}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) V^{k}\\left(s^{\\prime}\\right)\\right) Vk+1(s)=a∈A∑​π(a∣s)(r(s,a)+γs′∈S∑​P(s′∣s,a)Vk(s′)) 策略提升 在当前状态采取新动作得到的价值大于当前状态价值，因此可以更新策略，获取更高价值。 Vπ′(s)≥Vπ(s)V^{\\pi^{\\prime}}(s) \\geq V^{\\pi}(s) Vπ′(s)≥Vπ(s)","tags":["rl"],"categories":["rl"]},{"title":"2024数学建模国赛","path":"/2024/09/03/计算机基础/数学建模2024/","content":"第二问思路 是否对配件检验： 如果对配件检验：得到的配件一定是合格品，只需计算平均价值。 E=(w+c)pE = \\frac{(w+c)}{p}E=p(w+c)​ ppp表示合格品概率，www表示配件购买价格，ccc表示配件检验成本 不对配件检验：会提高成品的不合格率 是否对成品检验： 对成品检验：需要付出检验费用，但卖出的成品一定是合格品，所以不需要支付退还费用。 成本构成：1.合格品的成本（包括配件购买费用+配件检验费用） ；2.组装费用； 3.成品检验费用 ；4.不合格品的成本 对成品不检验：不需要检验费用，但卖出的成品有一部分是不合格品，需要支付退还费用。 成本构成： 枚举所有做法（第二问） 首先考虑不拆解的情况 不检验配件A，不检验配件B，不检验成品，不拆解成品。 成品为合格品概率：p(qualified)=pa∗pb∗pcp(qualified) = p_a * p_b * p_cp(qualified)=pa​∗pb​∗pc​ 不合格品概率：1−p(qualified)1 - p(qualified)1−p(qualified) 平均收益：E=p(qualified)∗w−1∗(cost)−(1−p(qualified))∗(swap)E = p(qualified) * w - 1 * (cost) - (1-p(qualified))*(swap)E=p(qualified)∗w−1∗(cost)−(1−p(qualified))∗(swap) 检验配件A，不检验配件B，不检验成品，不拆解成品。 配件A的平均成本：cost+cp\\frac{cost + c}{p}pcost+c​ 成品为合格品概率：p(qualified)=pb∗pcp(qualified) = p_b * p_cp(qualified)=pb​∗pc​ 平均收益：E=p(qualified)∗w−1∗cost−(1−qualified)∗swapE = p(qualified) * w - 1*cost - (1-qualified)*swapE=p(qualified)∗w−1∗cost−(1−qualified)∗swap 组成一件成品的价格：cost=costa+costb+evala+assemblecost = cost_a + cost_b + eval_a + assemblecost=costa​+costb​+evala​+assemble 不检验配件A，检验配件B，不检验成品，不拆解成品。 原理同上 检验配件A，检验配件B，不检验成品，不拆解成品。 合格品概率：p(qualified)=pcp(qualified) = p_cp(qualified)=pc​ 平均收益：E=p(qualified)∗w−1∗cost−(1−p(qualified))∗swapE = p(qualified) * w - 1 * cost - (1-p(qualified))*swapE=p(qualified)∗w−1∗cost−(1−p(qualified))∗swap 生产价格：cost=E(costA)+E(costB)+assemblecost = E(cost_A) + E(cost_B) + assemblecost=E(costA​)+E(costB​)+assemble 综上所述：当不检验成品，且不拆解成品时，卖出每件产品的期望收益为： E=p(qualified)∗w−1∗cost−p(unqualified)∗swapE = p(qualified) * w - 1*cost - p(unqualified) * swapE=p(qualified)∗w−1∗cost−p(unqualified)∗swap 只是每种情况，计算生产成本的结果不同 不检验配件A，不检验配件B，检验成品，不拆解成品。 合格品概率：p(qualified)=pA∗pB∗pCp(qualified) = p_A * p_B * p_Cp(qualified)=pA​∗pB​∗pC​ 销售收益：p(qualified)∗wp(qualified) * wp(qualified)∗w 生产成本：costA+costB+assemble+evalCcost_A + cost_B + assemble + eval_CcostA​+costB​+assemble+evalC​ 检验配件A，不检验配件B，检验成品，不拆解成品。 合格品概率：p(qualified)=pB∗pCp(qualified) = p_B * p_Cp(qualified)=pB​∗pC​ 销售收益：p(qualified)∗wp(qualified) * wp(qualified)∗w 成产成本：E(costA)+costB+assemble+evalCE(cost_A) + cost_B + assemble + eval_CE(costA​)+costB​+assemble+evalC​ 平均收益：E=p(qualified)∗w−1∗costE = p(qualified) * w - 1*costE=p(qualified)∗w−1∗cost 不检验配件A，检验配件B，检验成品，不拆解成品 同6 检验配件A，检验配件B，检验成品，不拆解成品 cost=E(costA)+E(costB)+assemble+evalCcost = E(cost_A) + E(cost_B) + assemble + eval_Ccost=E(costA​)+E(costB​)+assemble+evalC​ 综上所述：不拆解成品的情况下，所有方案 卖出一件的预期收益为： E=p(qualified)∗w−1∗cost−p(unqualified)∗swapE = p(qualified) * w - 1 * cost - p(unqualified)*swapE=p(qualified)∗w−1∗cost−p(unqualified)∗swap 是否检验配件，影响costcostcost的计算； 是否检验成品，决定了售出成品的不合格率是否为0，可以减少退还损失。 其次考虑拆解成品的情况 由于拆解的是检验不合格的成品，所以，当拆解成品时，必然伴随着检验成品。 不检验配件A，不检验配件B，检验成品，且拆解成品，且不进行二次检验。 暂时认为，这是一种很糟糕的策略，只有在次品率极低的情况下适用。 检验配件A，检验配件B，检验成品，且拆解成品，无需进行二次检验。 第i次合成成功的成本：cost(i)=E(costA)+E(costB)+i∗assemble+(i−1)∗meltcost(i) = E(cost_A) + E(cost_B) + i*assemble + (i-1)*meltcost(i)=E(costA​)+E(costB​)+i∗assemble+(i−1)∗melt 生产成本：$cost = \\sum_{i=1}^{\\infty} cost(i) $ 成品的合格率：p(qualified)=pcp(qualified) = p_cp(qualified)=pc​ 卖出一件产品的预期收益同上。 检验配件A，不检验配件B，检验成品，拆解成品，并对B进行二次检验 这里需要用到朴素贝叶斯公式！ 成品合格率：p(qualified)=pB∗pCp(qualified) = p_B * p_Cp(qualified)=pB​∗pC​ 仅因为组装问题导致为次品的概率为：(1−pC)∗pB(1-p_C)*p_B(1−pC​)∗pB​ ， 配件B损坏导致为次品的概率（包括了B既为次品，还组装失败的情况）：1−pB1-p_B1−pB​ 由于所有配件都至多检验一次，一旦通过检验，则一定为合格品。 第i次合成成功的花费： cost(i)=E(costA)+costB+i∗assemble+(i−1)∗melt+(i&gt;1)?:0:1∗evalccost(i) = E(cost_A) + cost_B + i*assemble + (i-1)*melt + (i&gt;1)?:0:1 * eval_ccost(i)=E(costA​)+costB​+i∗assemble+(i−1)∗melt+(i&gt;1)?:0:1∗evalc​ 生产成本： cost=pb∗pc∗cost(1)+(1−pb)∗(E(costA)+costB+assemble+evalc)+pb∗(1−pc)(i−1)∗pc∗(E(costA)+costB+i∗assemble+(i−1)∗melt)cost = p_b*p_c*cost(1) + (1-p_b)*(E(cost_A)+cost_B + assemble + eval_c) + p_b * (1-p_c)^{(i-1)} * p_c *(E_(cost_A) + cost_B + i*assemble + (i-1)*melt)cost=pb​∗pc​∗cost(1)+(1−pb​)∗(E(costA​)+costB​+assemble+evalc​)+pb​∗(1−pc​)(i−1)∗pc​∗(E(​costA​)+costB​+i∗assemble+(i−1)∗melt) 4.不检验A，B，检验成品，拆解成品，并对A，B进行二次检验。 cost=∑i=1ncosticost = \\sum_{i=1}^{n} cost_i cost=i=1∑n​costi​","categories":["其他"]},{"title":"rs232","path":"/2024/08/05/fpga/rs232/","content":"理论学习 通信协议基础 ​ 通用异步收发传输器（Universal Asynchronous Receiver/Transmitter），通常称作UART。UART是一种通用的数据通信协议，也是异步串行通信口（串口）的总称，它在发送数据时将并行数据转换成串行数据来传输，在接收数据时将接收到的串行数据转换成并行数据。它包括了RS 232、RS499、RS423、RS422和RS485等接口标准规范和总线标准规范。 ​ 串口作为常用的三大低速总线（UART、SPI、IIC）之一，在设计众多通信接口和调试时占有重要地位。但UART和SPI、IIC不同的是，它是异步通信接口，异步通信中的接收方并不知道数据什么时候会到达，所以双方收发端都要有各自的时钟，在数据传输过程中是不需要时钟的，发送方发送的时间间隔可以不均匀，接受 方是在数据的起始位和停止位的帮助下实现信息同步的。而SPI、IIC是同步通信接口（后面的章节会做详细介绍），同步通信中双方使用频率一致的时钟，在数据传输过程中时钟伴随着数据一起传输，发送方和接收方使用的时钟都是由主机提供的。 UART基础 ​ UART通信只有两根信号线，一根是发送数据端口线叫tx（Transmitter），一根是接收数据端口线叫rx（Receiver），对于PC来说它的tx要和对于FPGA来说的rx连接，同样PC的rx要和FPGA的tx连接。UART可以实现全双工，即可以同时进行发送数据和接收数据。 RS232优点 ​ 串口RS232传输数据的距离虽然不远，传输速率也相对较慢，但是串口依然被广泛的用于电路系统的设计中，串口的好处主要表现在以下几个方面： 很多传感器芯片或CPU都带有串口功能，目的是在使用一些传感器或CPU时可以通过串口进行调试，十分方便； 在较为复杂的高速数据接口和数据链路集合的系统中往往联合调试比较困难，可以先使用串口将数据链路部分验证后，再把串口换成高速数据接口。如在做以太网相关的项目时，可以在调试时先使用串口把整个数据链路调通，然后再把串口换成以太网的接口； 串口的数据线一共就两根，也没有时钟线，节省了大量的管脚资源。 RS232信号线 ​ 在旧式的台式计算机中一般会有 RS-232 标准的 COM 口(也称 DB9 接口)。 名称 符号 数据方向 说明 载波检测 DCD DTE→DCE Data Carrier Detect,数据载波检测，用于 DTE告知对方，本机是否收到对方的载波信 号 接收数据 RXD DTE&lt;DCE ReceiveData,数据接收信号，即输入。 发送数据 TXD DTE→DCE Transmit Data,数据发送信号，即输出。两个 设备之间的TXD与RXD应交叉相连 数据终端 (DTE)就 绪 DTR DTE→DCE Data Terminal Ready,数据终端就绪，用于 DTE向对方告知本机是否已准备好 信号地 GND 地线，两个通讯设备之间的地电位可能不一 样，这会影响收发双方的电平信号，所以两 个串口设备之间必须要使用地线连接，即共 地。 数据设备 (DCE)就 绪 DSR DTE&lt;DCE Data Set Ready,数据发送就绪，用于DCE告 知对方本机是否处于待命状态 请求发送 RTS DTE→DCE Request To Send,请求发送，DTE请求DCE 本设备向DCE端发送数据 允许发送 CTS DTE&lt;DCE Clear To Send,允许发送，DCE回应对方的 RTS发送请求，告知对方是否可以发送数据 响铃指示 RI DTE&lt;DCE Ring Indicator,响铃指示，表示DCE端与线 路已接通 RS232协议基础 RS232帧结构 ​\t在没有数据传输时，信道持续传输1，知道遇到一位0（起始位），标志着一帧数据的传输。 波特率：在信息传输通道中，携带数据信息的信号单元叫码元（因为串口是1bit进行传输的，所以其码元就是代表一个二进制数），每秒钟通过信号传输的码元数称为码元的传输速率，简称波特率，常用符号“Baud”表示，其单位为“波特每秒（Bps）”。串口常见的波特率有4800、9600、115200等。 比特率：每秒钟通信信道传输的信息量称为位传输速率，简称比特率，其单位为“每秒比特数（bps）”。比特率可由波特率计算得出，公式为：比特率=波特率 * 单个调制状态对应的二进制位数。如果使用的是9600的波特率，其串口的比特率为：9600Bps * 1bit= 9600bps。 由计算得串口发送或者接收1bit数据的时间为一个波特，即1/9600秒，如果用50MHz（周期为20ns）的系统时钟来计数，需要计数的个数为cnt = (1s * 10^9)ns / 9600bit)ns / 20ns ≈ 5208个系统时钟周期，即每个bit数据之间的间隔要在50MHz的时钟频率下计数5208次。 项目实战 RS232 接收端模块 该模块功能为：将串行数据转化为8位并行数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124module uart_rx // rs232接受模块 ： 需要把串行数据转化为并行数据#(\tparameter UART_BPS = &#x27;d9600, //串口波特率\tparameter CLK_FREQ = &#x27;d50_000_000 //时钟频率)(\tinput sys_clk,sys_rst_n, input rx,\toutput reg po_flag,\toutput reg [7:0] po_data);parameter BAUD_CNT_MAX = CLK_FREQ / UART_BPS;reg rx_reg1,rx_reg2,rx_reg3;\t// 将po_data打两拍消除亚稳态，reg3再打一拍检测下降沿（起始位）reg start_neg;\t// 数据开始标志位reg work_en;\t// 工作状态reg [12:0] baud_cnt;\t// 波特计数器reg bit_flag; // bit加信号reg [3:0] bit_cnt; // bit计数器reg rx_flag; // 输出传输完成信号reg [7:0] rx_data; // 数据移位寄存器// 第一级寄存器，复位状态为1 : // 当没有数据传输时，信道一直传输1，直到发现起始位0，产生一个下降沿，表示数据传输开始always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg1 &lt;= 1&#x27;b1;\telse rx_reg1 &lt;= rx;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg2 &lt;= 1&#x27;b1;\telse rx_reg2 &lt;= rx_reg1;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg3 &lt;= 1&#x27;b1;\telse rx_reg3 &lt;= rx_reg2;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) start_neg &lt;= 1&#x27;b0;\telse if(rx_reg2 == 1&#x27;b0 &amp;&amp; rx_reg3 == 1&#x27;b1 &amp;&amp; work_en == 1&#x27;b0) start_neg &lt;= 1&#x27;b1;\telse start_neg &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) work_en &lt;= 1&#x27;b0;\telse if(start_neg == 1&#x27;b1) work_en &lt;= 1&#x27;b1;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) work_en &lt;= 1&#x27;b0;\telse work_en &lt;= work_en;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) baud_cnt &lt;= 13&#x27;b0;\telse if(work_en == 1&#x27;b1) begin if(baud_cnt == BAUD_CNT_MAX - 1) baud_cnt &lt;= 13&#x27;b0; else baud_cnt &lt;= baud_cnt + 1; end\telse baud_cnt &lt;= 13&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_flag &lt;= 1&#x27;b0;\telse if(baud_cnt == (BAUD_CNT_MAX &gt;&gt; 1)) bit_flag &lt;= 1&#x27;b1;\telse bit_flag &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_flag == 1&#x27;b1) begin if(bit_cnt == 4&#x27;d8) bit_cnt &lt;= 4&#x27;b0; else bit_cnt &lt;= bit_cnt + 1; end\telse bit_cnt &lt;= bit_cnt;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_data &lt;= 8&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; (bit_cnt &gt;= 1 &amp;&amp; bit_cnt &lt;= 8)) rx_data &lt;= &#123;rx_reg3,rx_data[7:1]&#125;;\telse rx_data &lt;= rx_data;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_flag &lt;= 1&#x27;b0;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) rx_flag &lt;= 1&#x27;b1;\telse rx_flag &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_data &lt;= 8&#x27;b0;\telse if(rx_flag == 1&#x27;b1) po_data &lt;= rx_data;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_flag &lt;= 1&#x27;b0;\telse if(rx_flag == 1&#x27;b1) po_flag &lt;= rx_flag;endmodule RS232发送端模块 该模块功能为：将8位并行数据转化为串行数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980module uart_tx\t// rs232发送模块，把并行数据转化为串行#(\tparameter UART_BPS = &#x27;d9600,\tparameter CLK_FREQ = &#x27;d50_000_000)(\tinput sys_clk,sys_rst_n, input pi_flag, input [7:0] pi_data,\toutput reg tx );parameter BAUD_CNT_MAX = CLK_FREQ / UART_BPS;reg [12:0] baud_cnt;reg bit_flag;reg [3:0] bit_cnt;reg work_en;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) work_en &lt;= 1&#x27;b0;\telse if(pi_flag == 1&#x27;b1) work_en &lt;= 1&#x27;b1;\telse if(bit_cnt == 4&#x27;d9 &amp;&amp; bit_flag == 1&#x27;b1) work_en &lt;= 1&#x27;b0;\telse work_en &lt;= work_en;always@(posedge sys_clk or negedge sys_rst_n) if(sys_rst_n == 1&#x27;b0) baud_cnt &lt;= 13&#x27;b0; else if(work_en == 1&#x27;b1) begin if(baud_cnt == BAUD_CNT_MAX - 1) baud_cnt &lt;= 13&#x27;b0; else baud_cnt &lt;= baud_cnt + 1; end else baud_cnt &lt;= 13&#x27;b0; always@(posedge sys_clk or negedge sys_rst_n) if(sys_rst_n == 1&#x27;b0) bit_flag &lt;= 1&#x27;b0; else if(baud_cnt == 1) bit_flag &lt;= 1&#x27;b1; else bit_flag &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; bit_cnt == 4&#x27;d9) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; work_en == 1&#x27;b1) bit_cnt &lt;= bit_cnt + 1;\telse bit_cnt &lt;= bit_cnt;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) tx &lt;= 1&#x27;b1;\telse if(bit_flag == 1&#x27;b1) case(bit_cnt) 0 : tx &lt;= 1&#x27;b0; 1 : tx &lt;= pi_data[0]; 2 : tx &lt;= pi_data[1]; 3 : tx &lt;= pi_data[2]; 4 : tx &lt;= pi_data[3]; 5 : tx &lt;= pi_data[4]; 6 : tx &lt;= pi_data[5]; 7 : tx &lt;= pi_data[6]; 8 : tx &lt;= pi_data[7]; 9 : tx &lt;= 1&#x27;b1; default : tx &lt;= 1&#x27;b1; endcaseendmodule 学习链接 笔记摘自：野火fpga开发实战指南：串口RS232 其他： 详解 | 还不懂串口通信？看这篇！","categories":["fpga"]},{"title":"rs485","path":"/2024/08/05/fpga/rs485/","content":"理论基础 RS-485是双向、半双工通信协议，信号采用差分传输方式，允许多个驱动器和接收器挂接在总线上，其中每个驱动器都能够脱离总线。 RS-232是双向、全双工通信协议，信号采用单端传输方式。 差分传输有更好的抗干扰能力。 其次，485相对232可以进行长距离的信号传输；使用收发器，对电压敏感（200mv），传输距离（1200m），传输最大速率（10mb/s）。但485只支持半双工。","categories":["fpga"]},{"title":"基于rs232的lcd图像显示","path":"/2024/08/05/fpga/uart_lcd/","content":"1.使用uart_rx模块作为图像接受模块 波特率的计算 波特率：每秒钟通过信号传输的码元数称为码元的传输速率 使用时钟频率 除以 波特率 可以得到传输每个码源所占用的时钟周期数 $baud_cnt_max = \\frac{时钟频率}{波特率} $ 为什么要做打两拍操作 消除亚稳态，稳定信号。 在找下降沿时，不能使用rx_reg1rx\\_reg1rx_reg1 和rx_reg2rx\\_reg2rx_reg2 求得，因为rx_reg1rx\\_reg1rx_reg1 信号不稳定，使用其求得的结果也不稳定，因此将 rx_reg2rx\\_reg2rx_reg2再打一拍，使用rx_reg2rx\\_reg2rx_reg2 和 rx_reg3rx\\_reg3rx_reg3 求得。 代码中 bit_cntbit\\_cntbit_cnt 只计数到8 这是因为没有计数停止位，所以只有0~8 共计数 9位（1位起始位+8位数据位）。 rs232数据移位方向： 最先传输的比特在低位，最后传输的比特在高位 PC机通过串口调试助手往FPGA发8bit数据时，FPGA通过串口线rx一位一位地接收，从最低位到最高位依次接收，最后在FPGA里面位拼接成8比特数据。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119module uart_rx#parameter(\tparameter UART_BPS = &#x27;d9600, CLK_FREQ = &#x27;d50_000_000)(\tinput sys_clk,sys_rst_n, input rx, output reg [7:0] po_data,\toutput reg po_flag);localparam BAUD_CNT_MAX = CLK_FREQ / UART_BPS;reg rx_reg1;reg rx_reg2;reg rx_reg3;reg start_nedge;reg work_en;reg [12:0] baud_cnt;reg bit_flag;reg [3:0] bit_cnt;reg [7:0] rx_data;reg rx_flag;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg1 &lt;= 1&#x27;b0;\telse rx_reg1 &lt;= rx;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg2 &lt;= 1&#x27;b0;\telse rx_reg2 &lt;= rx_reg1;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_reg3 &lt;= 1&#x27;b0;\telse rx_reg3 &lt;= rx_reg2;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) start_nedge &lt;= 1&#x27;b0;\telse if(rx_reg2 == 1&#x27;b0 &amp;&amp; rx_reg3 == 1&#x27;b1) start_nedge &lt;= 1&#x27;b1;\telse start_nedge &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) work_en &lt;= 1&#x27;b0;\telse if(start_nedge == 1&#x27;b1)\t// 开始工作条件 work_en &lt;= 1&#x27;b1;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) // 停止工作条件：完成一个字节的传输 work_en &lt;= 1&#x27;b0;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) baud_cnt &lt;= 13&#x27;b0;\telse if(baud_cnt == BAUD_CNT_MAX - 1 &amp;&amp; work_en == 1&#x27;b1) baud_cnt &lt;= 13&#x27;b0;\telse if(work_en == 1&#x27;b1) baud_cnt &lt;= baud_cnt + 1;\telse baud_cnt &lt;= 13&#x27;b0;\t// 传输完成，停止工作，波特计数器清零always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_flag &lt;= 1&#x27;b0;\telse if(baud_cnt == BAUD_CNT_MAX / 2 - 1) bit_flag &lt;= 1&#x27;b1;\telse bit_flag &lt;= 1&#x27;b0;// 这里由于没有计数停止位，所以只对0~8计数// 传输停止位时已经将work_en拉低，不会使bit_flag有效，因此bit_cnt也不会计数always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) bit_cnt &lt;= 4&#x27;b0;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1) bit_cnt &lt;= 4&#x27;d0;\telse if(bit_flag == 1&#x27;b1) bit_cnt &lt;= bit_cnt + 1;// 对数据进行移位always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_data &lt;= 8&#x27;b0;\telse if(bit_flag == 1&#x27;b1 &amp;&amp; bit_cnt &gt;= 4&#x27;d1 &amp;&amp; bit_cnt &lt;= 4&#x27;d8) rx_data = &#123;rx_reg3,rx_data[7:1]&#125;;// 传输完8比特，拉高rx_flagalways@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) rx_flag &lt;= 1&#x27;b0;\telse if(bit_cnt == 4&#x27;d8 &amp;&amp; bit_flag == 1&#x27;b1)\t// 8位的最后一个比特传输完成 rx_flag &lt;= 1&#x27;b1;\telse rx_flag &lt;= 1&#x27;b0;// po_data 和 po_flag 都以 rx_data 为基准，可以实现统一时间输出always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_data &lt;= 8&#x27;b0;\telse if(rx_flag == 1&#x27;b1) po_data &lt;= rx_data;always@(posedge sys_clk or negedge sys_rst_n)\tif(sys_rst_n == 1&#x27;b0) po_flag &lt;= 1&#x27;b0;\telse po_flag &lt;= rx_flag;endmodule","categories":["fpga"]},{"title":"Sublime快捷键","path":"/2024/08/03/工具使用基础/sublime快捷键/","content":"Sublime快捷键 查找替换 查找 ： Ctrl + F 查找替换 ：Ctrl + H","categories":["计算机基础"]},{"title":"Git基础","path":"/2024/08/03/计算机基础/Git/","content":"一.git基础 初始化git仓库 1git init 添加文件 1git add readme.txt 提交文件 1git commit -m &quot;submit infomation&quot; 查看工作区状态 1git status 查看修改内容 1git diff 查看提交历史 12git loggit reflog 回退版本 1git reset --hard 7376(版本号) 撤销暂存区的修改 1git reset HEAD readme.txt 撤销工作区的修改 1git checkout -- readme.txt 删除版本库的文件 1git rm readme.txt 管理github仓库 123git remote add origin git@github.com:skyang1/learngit.gitgit remote -v // 查看关联仓库 推送本地库的所有内容到github 1git push -u origin master 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 克隆github项目 1git clone git@github.com:LeiWang1999/FPGA.git 二、分支管理 查看分支 1git branch 创建分支 1git branch dev 切换分支 12git switch devgit checkout dev 创建并切换分支 12git switch -c devgit checkout -b dev 合并某分支到当前分支 1git merge dev 删除分支 1git branch -d dev","tags":["git","linux"],"categories":["计算机基础"]},{"title":"我的朋友们","path":"/friends/index.html","content":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"path":"/about/index.html","content":"下面写关于自己的内容"},{"title":"Hexo-stellar配置","path":"/wiki/diary/example.html","content":"今天主要问题在文档的配置，按照官方教程无法配置成功，更改项目结构后配置成功。 新的项目结构为： wiki.yml ：文档目录，存放在source/_data目录下- diary 1- diary diary.yml ：日记文档的配置文件，存放到source/_data/wiki目录下 123456789101112131415161718192021222324252627name: Diarytitle: skyang-学习日记subtitle: &#x27;每个人的独立博客 | Designed by xaoxuu&#x27;tags: 日记icon: /assets/wiki/stellar/icon.svgcover: /assets/wiki/stellar/icon.svgdescription: skyang学习日记-每天学习的一些零碎的知识，难以整理，写为日记# repo: xaoxuu/hexo-theme-stellarsearch: filter: /wiki/stellar/ placeholder: 在 Stellar 中搜索...leftbar: - tree - timeline_stellar_releases - related# comment_title: &#x27;评论区仅供交流，有问题请提 [issue](https://github.com/xaoxuu/hexo-theme-stellar/issues) 反馈。&#x27;# comments:# service: giscus# giscus:# data-repo: xaoxuu/hexo-theme-stellar# data-mapping: number# data-term: 226base_dir: /wiki/stellar/tree: &#x27;快速开始&#x27;: - index\t- day_20241210 day_20241210.md ：文档具体内容，存放在myblog/wiki/diary路径下"},{"title":"日记首页","path":"/wiki/diary/index.html","content":"记录一下每日学习日常，以零散笔记为主。"},{"title":"npm安装软件包","path":"/wiki/diary/npm.html","content":"npm安装软件包 全局安装 1nmp install hexo -g 安装的软件包在NodeJS安装目录下的global_modules文件夹下 本地安装 安装到当前路径的文件夹下 由于没有配置全局环境变量，可以使用 npx 指令使用安装的软件包 1npx hexo g -d"},{"title":"Hexo-stellar配置","path":"/wiki/diary/stellar.html","content":"今天主要问题在文档的配置，按照官方教程无法配置成功，更改项目结构后配置成功。 新的项目结构为： wiki.yml ：文档目录，存放在source/_data目录下- diary 1- diary diary.yml ：日记文档的配置文件，存放到source/_data/wiki目录下 123456789101112131415161718192021222324252627name: Diarytitle: skyang-学习日记subtitle: &#x27;每个人的独立博客 | Designed by xaoxuu&#x27;tags: 日记icon: /assets/wiki/stellar/icon.svgcover: /assets/wiki/stellar/icon.svgdescription: skyang学习日记-每天学习的一些零碎的知识，难以整理，写为日记# repo: xaoxuu/hexo-theme-stellarsearch: filter: /wiki/stellar/ placeholder: 在 Stellar 中搜索...leftbar: - tree - timeline_stellar_releases - related# comment_title: &#x27;评论区仅供交流，有问题请提 [issue](https://github.com/xaoxuu/hexo-theme-stellar/issues) 反馈。&#x27;# comments:# service: giscus# giscus:# data-repo: xaoxuu/hexo-theme-stellar# data-mapping: number# data-term: 226base_dir: /wiki/stellar/tree: &#x27;快速开始&#x27;: - index\t- day_20241210 day_20241210.md ：文档具体内容，存放在myblog/wiki/diary路径下"},{"title":"java多线程编程","path":"/wiki/java/juc.html","content":"多线程基础 并发：在同一时刻，有多个指令在单个cpu上交替执行。 并行：在同一时刻，有多个指令在多个cpu上同时执行。 多线程实现方式 继承Thread类方式实现 定义一个类继承Thread 重写子类run()方法 创建子类对象并调用进程 12345678public class Mythread extends Thread&#123; @Override public void run() &#123; for(int i=0;i&lt;100;i++)&#123; System.out.println(this.getName() + i); &#125; &#125;&#125; 123456789101112public class Main &#123; public static void main(String[] args)&#123; Mythread th1 = new Mythread(); Mythread th2 = new Mythread(); th1.setName(&quot;thread1:&quot;); th2.setName(&quot;thread2:&quot;); th1.start(); th2.start(); &#125;&#125; 实现Runnable接口实现 自己定义一个类实现Runnable接口 重写里面的run方法 创建自己的类的对象 创建个Thread类的对象，并开启线程 12345678public class Myrun implements Runnable&#123; @Override public void run() &#123; Thread th = Thread.currentThread(); for(int i=0;i&lt;100;i++) System.out.println(th.getName() + i); &#125;&#125; 1234567891011121314151617public class Main &#123; public static void main(String[] args)&#123; // 创建任务对象，表示多线程要执行的任务 Myrun run= new Myrun(); // 创建线程对象 Thread th1 = new Thread(run); Thread th2 = new Thread(run); th1.setName(&quot;Thread1:&quot;); th2.setName(&quot;Thread2:&quot;); th1.start(); th2.start(); &#125;&#125; 使用Callable接口和Furture接口进行实现 创建一个类MyCallab1e实现callab1e按口 重写call(是有返回值的，表示多线程运行的结果) 创建MyCallable的对象（表示多线程要执行的任务） 创建FutureTask的对象（作用管理多线程运行的结果） 创建Thread类的对象，并启动（表示线程） 1234567891011public class Mycallable implements Callable &#123; @Override public Object call() throws Exception &#123; int sum = 0; for(int i=0;i&lt;100;i++)&#123; System.out.println(i); sum = sum + i; &#125; return sum; &#125;&#125; 12345678910111213141516public class Main &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 创建MyCallable对象，表示多线程要执行的任务 Mycallable mc = new Mycallable(); // 创建FureturTask对象，管理多线程运行的结果 FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread th = new Thread(ft); th.start(); // 获取多线程运行的结果 Integer result = ft.get(); System.out.println(result); &#125;&#125; Thread常用成员方法 优先级：1~10，默认为5，仅代表运行概率。 守护线程：当其他的非守护线程执行完毕之后：守护线程会陆续结束 线程生命周期 案例一 模拟使用三个线程表示三个售票窗口，一共出售1000张门票。 Thread实现 注意点： synchronized 应当锁一个相同的对象，可以锁一个静态成员变量 / 本类的class字节码文件。 synchronized应当放到循环之内，否则一个线程循环1000次后才会解锁。 123456789101112131415161718192021package org.example.juc;public class Mythread extends Thread&#123; static int ticket = 0; @Override public void run() &#123; while(true) &#123; synchronized (Mythread.class)&#123; if(ticket == 1000) break; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; ticket++; System.out.println(Thread.currentThread().getName() + &quot;出售了门票&quot; + ticket); &#125; &#125; &#125;&#125; Runnable实现 期间的问题：由于线程切换速度较慢，所以出售100张票可能看不到线程切换的表现，因此将循环次数改为1000。 1234567891011121314151617181920212223public class Myrun implements Runnable&#123; static int ticket = 0; @Override public void run() &#123; while(true) if(method()) break; &#125; private synchronized boolean method()&#123; if(ticket == 1000) return true; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; ticket++; System.out.println(Thread.currentThread().getName() + &quot;出售了门票&quot; + ticket); return false; &#125;&#125; 案例二 普通实现 生产者消费者问题：一位顾客吃面，一位厨师做面，桌子上可以摆一碗面。","tags":[null],"categories":[null]}]